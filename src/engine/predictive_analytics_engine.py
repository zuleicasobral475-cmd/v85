#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Predictive Analytics Engine
Motor de An√°lise Preditiva e Insights Profundos Ultra-Avan√ßado
"""

import os
import logging
import json
import asyncio
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter, defaultdict
import re
import warnings
warnings.filterwarnings('ignore')

# Imports condicionais para an√°lise avan√ßada
try:
    import spacy
    HAS_SPACY = True
except ImportError:
    HAS_SPACY = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    from textblob import TextBlob
    HAS_TEXTBLOB = True
except ImportError:
    HAS_TEXTBLOB = False

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    HAS_VADER = True
except ImportError:
    HAS_VADER = False

try:
    import gensim
    from gensim import corpora, models
    HAS_GENSIM = True
except ImportError:
    HAS_GENSIM = False

try:
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False

try:
    import cv2
    HAS_OPENCV = True
except ImportError:
    HAS_OPENCV = False

try:
    from prophet import Prophet
    HAS_PROPHET = True
except ImportError:
    HAS_PROPHET = False

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    HAS_PLOTLY = True
except ImportError:
    HAS_PLOTLY = False

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

from services.auto_save_manager import salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class PredictiveAnalyticsEngine:
    """Motor de An√°lise Preditiva e Insights Profundos Ultra-Avan√ßado"""

    def __init__(self):
        """Inicializa o motor de an√°lise preditiva"""
        self.nlp_model = None
        self.sentiment_analyzer = None
        self.tfidf_vectorizer = None
        self.topic_model = None
        
        # Configura√ß√µes de an√°lise
        self.config = {
            'min_text_length': 100,
            'max_features_tfidf': 1000,
            'n_topics_lda': 10,
            'n_clusters_kmeans': 5,
            'confidence_threshold': 0.7,
            'prediction_horizon_days': 90,
            'min_data_points_prediction': 5
        }
        
        self._initialize_models()
        logger.info("üîÆ Predictive Analytics Engine Ultra-Avan√ßado inicializado")

    def _initialize_models(self):
        """Inicializa modelos de ML e NLP"""
        
        # Carrega modelo SpaCy para portugu√™s
        if HAS_SPACY:
            try:
                self.nlp_model = spacy.load("pt_core_news_sm")
                logger.info("‚úÖ Modelo SpaCy portugu√™s carregado")
            except OSError:
                try:
                    self.nlp_model = spacy.load("pt_core_news_lg")
                    logger.info("‚úÖ Modelo SpaCy portugu√™s (large) carregado")
                except OSError:
                    logger.warning("‚ö†Ô∏è Modelo SpaCy n√£o encontrado. Execute: python -m spacy download pt_core_news_sm")
                    self.nlp_model = None
        
        # Inicializa analisador de sentimento
        if HAS_VADER:
            self.sentiment_analyzer = SentimentIntensityAnalyzer()
            logger.info("‚úÖ Analisador de sentimento VADER carregado")
        
        # Inicializa TF-IDF
        if HAS_SKLEARN:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=self.config['max_features_tfidf'],
                stop_words=self._get_portuguese_stopwords(),
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
            logger.info("‚úÖ TF-IDF Vectorizer configurado")

    def _get_portuguese_stopwords(self) -> List[str]:
        """Retorna lista de stopwords em portugu√™s"""
        return [
            'a', 'o', 'e', '√©', 'de', 'do', 'da', 'em', 'um', 'uma', 'para', 'com', 'n√£o', 'que', 'se', 'na', 'por',
            'mais', 'as', 'os', 'como', 'mas', 'foi', 'ao', 'ele', 'das', 'tem', '√†', 'seu', 'sua', 'ou', 'ser',
            'quando', 'muito', 'h√°', 'nos', 'j√°', 'est√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso',
            'ela', 'entre', 'era', 'depois', 'sem', 'mesmo', 'aos', 'ter', 'seus', 'quem', 'nas', 'me', 'esse',
            'eles', 'est√£o', 'voc√™', 'tinha', 'foram', 'essa', 'num', 'nem', 'suas', 'meu', '√†s', 'minha', 't√™m',
            'numa', 'pelos', 'elas', 'havia', 'seja', 'qual', 'ser√°', 'n√≥s', 'tenho', 'lhe', 'deles', 'essas',
            'esses', 'pelas', 'este', 'fosse', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus', 'minhas'
        ]

    async def analyze_session_data(self, session_id: str) -> Dict[str, Any]:
        """
        Analisa todos os dados dispon√≠veis de uma sess√£o para gerar insights preditivos ultra-avan√ßados
        
        Args:
            session_id: ID da sess√£o
            
        Returns:
            Dict com insights preditivos completos
        """
        logger.info(f"üîÆ INICIANDO AN√ÅLISE PREDITIVA ULTRA-AVAN√áADA para sess√£o: {session_id}")
        
        session_dir = Path(f"analyses_data/{session_id}")
        if not session_dir.exists():
            logger.error(f"‚ùå Diret√≥rio da sess√£o n√£o encontrado: {session_dir}")
            return {"success": False, "error": "Diret√≥rio da sess√£o n√£o encontrado"}

        # Estrutura de insights ultra-completa
        insights = {
            "session_id": session_id,
            "analysis_timestamp": datetime.now().isoformat(),
            "success": True,
            "methodology": "ARQV30_PREDICTIVE_ULTRA_v3.0",
            
            # An√°lises principais
            "textual_insights": {},
            "temporal_trends": {},
            "visual_insights": {},
            "network_analysis": {},
            "sentiment_dynamics": {},
            "topic_evolution": {},
            "engagement_patterns": {},
            
            # Previs√µes e cen√°rios
            "predictions": {},
            "scenarios": {},
            "risk_assessment": {},
            "opportunity_mapping": {},
            
            # M√©tricas de confian√ßa
            "confidence_metrics": {},
            "data_quality_assessment": {},
            
            # Recomenda√ß√µes estrat√©gicas
            "strategic_recommendations": {},
            "action_priorities": {}
        }

        try:
            # FASE 1: An√°lise Textual Ultra-Profunda
            logger.info("üß† FASE 1: An√°lise textual ultra-profunda...")
            insights["textual_insights"] = await self._perform_ultra_textual_analysis(session_dir)
            
            # FASE 2: An√°lise de Tend√™ncias Temporais
            logger.info("üìà FASE 2: An√°lise de tend√™ncias temporais...")
            insights["temporal_trends"] = await self._perform_temporal_analysis(session_dir)
            
            # FASE 3: An√°lise Visual Avan√ßada (OCR + Computer Vision)
            logger.info("üëÅÔ∏è FASE 3: An√°lise visual avan√ßada...")
            insights["visual_insights"] = await self._perform_advanced_visual_analysis(session_dir)
            
            # FASE 4: An√°lise de Rede e Conectividade
            logger.info("üï∏Ô∏è FASE 4: An√°lise de rede e conectividade...")
            insights["network_analysis"] = await self._perform_network_analysis(session_dir)
            
            # FASE 5: Din√¢mica de Sentimentos
            logger.info("üí≠ FASE 5: An√°lise de din√¢mica de sentimentos...")
            insights["sentiment_dynamics"] = await self._analyze_sentiment_dynamics(session_dir)
            
            # FASE 6: Evolu√ß√£o de T√≥picos
            logger.info("üîÑ FASE 6: An√°lise de evolu√ß√£o de t√≥picos...")
            insights["topic_evolution"] = await self._analyze_topic_evolution(session_dir)
            
            # FASE 7: Padr√µes de Engajamento
            logger.info("üìä FASE 7: An√°lise de padr√µes de engajamento...")
            insights["engagement_patterns"] = await self._analyze_engagement_patterns(session_dir)
            
            # FASE 8: Gera√ß√£o de Previs√µes Ultra-Avan√ßadas
            logger.info("üîÆ FASE 8: Gera√ß√£o de previs√µes ultra-avan√ßadas...")
            insights["predictions"] = await self._generate_ultra_predictions(insights)
            
            # FASE 9: Modelagem de Cen√°rios Complexos
            logger.info("üó∫Ô∏è FASE 9: Modelagem de cen√°rios complexos...")
            insights["scenarios"] = await self._model_complex_scenarios(insights)
            
            # FASE 10: Avalia√ß√£o de Riscos e Oportunidades
            logger.info("‚öñÔ∏è FASE 10: Avalia√ß√£o de riscos e oportunidades...")
            insights["risk_assessment"] = await self._assess_risks_and_opportunities(insights)
            
            # FASE 11: Mapeamento de Oportunidades
            logger.info("üéØ FASE 11: Mapeamento estrat√©gico de oportunidades...")
            insights["opportunity_mapping"] = await self._map_strategic_opportunities(insights)
            
            # FASE 12: M√©tricas de Confian√ßa
            logger.info("üìè FASE 12: C√°lculo de m√©tricas de confian√ßa...")
            insights["confidence_metrics"] = await self._calculate_confidence_metrics(insights)
            
            # FASE 13: Avalia√ß√£o de Qualidade dos Dados
            logger.info("üîç FASE 13: Avalia√ß√£o de qualidade dos dados...")
            insights["data_quality_assessment"] = await self._assess_data_quality(session_dir)
            
            # FASE 14: Recomenda√ß√µes Estrat√©gicas
            logger.info("üí° FASE 14: Gera√ß√£o de recomenda√ß√µes estrat√©gicas...")
            insights["strategic_recommendations"] = await self._generate_strategic_recommendations(insights)
            
            # FASE 15: Prioriza√ß√£o de A√ß√µes
            logger.info("üéØ FASE 15: Prioriza√ß√£o de a√ß√µes...")
            insights["action_priorities"] = await self._prioritize_actions(insights)

            # Salva insights preditivos
            insights_path = session_dir / "insights_preditivos.json"
            with open(insights_path, 'w', encoding='utf-8') as f:
                json.dump(insights, f, ensure_ascii=False, indent=2)
            
            # Salva tamb√©m como etapa
            salvar_etapa("insights_preditivos_completos", insights, categoria="analise_preditiva")
            
            logger.info(f"‚úÖ AN√ÅLISE PREDITIVA ULTRA-AVAN√áADA CONCLU√çDA: {insights_path}")
            return insights

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico na an√°lise preditiva: {e}")
            salvar_erro("predictive_analytics_critical", e, contexto={"session_id": session_id})
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    async def _perform_ultra_textual_analysis(self, session_dir: Path) -> Dict[str, Any]:
        """Realiza an√°lise textual ultra-profunda com NLP avan√ßado"""
        
        results = {
            "total_documents_processed": 0,
            "total_words_analyzed": 0,
            "entities_found": {},
            "key_topics": [],
            "sentiment_analysis": {},
            "linguistic_patterns": {},
            "emerging_themes": [],
            "semantic_clusters": {},
            "keyword_density": {},
            "readability_metrics": {},
            "emotional_indicators": {},
            "persuasion_elements": {}
        }

        # Coleta dados textuais
        textual_data = self._gather_comprehensive_textual_data(session_dir)
        results["total_documents_processed"] = len(textual_data)

        if not textual_data:
            logger.warning("‚ö†Ô∏è Nenhum dado textual encontrado para an√°lise")
            return results

        all_texts = []
        all_entities = []
        sentiment_scores = []
        
        # Processa cada documento
        for source, text_content in textual_data.items():
            if len(text_content) < self.config['min_text_length']:
                continue
                
            try:
                # An√°lise com SpaCy
                if HAS_SPACY and self.nlp_model:
                    doc = self.nlp_model(text_content[:1000000])  # Limita para performance
                    
                    # Extra√ß√£o de entidades nomeadas
                    for ent in doc.ents:
                        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'EVENT']:
                            all_entities.append((ent.text.strip(), ent.label_))
                    
                    # An√°lise de padr√µes lingu√≠sticos
                    linguistic_patterns = self._analyze_linguistic_patterns(doc)
                    results["linguistic_patterns"][source] = linguistic_patterns
                
                # An√°lise de sentimento
                if HAS_VADER and self.sentiment_analyzer:
                    sentiment = self.sentiment_analyzer.polarity_scores(text_content)
                    sentiment_scores.append(sentiment)
                    results["sentiment_analysis"][source] = sentiment
                
                # An√°lise de legibilidade
                readability = self._calculate_readability_metrics(text_content)
                results["readability_metrics"][source] = readability
                
                # Indicadores emocionais
                emotional_indicators = self._extract_emotional_indicators(text_content)
                results["emotional_indicators"][source] = emotional_indicators
                
                # Elementos de persuas√£o
                persuasion_elements = self._identify_persuasion_elements(text_content)
                results["persuasion_elements"][source] = persuasion_elements
                
                all_texts.append(text_content)
                results["total_words_analyzed"] += len(text_content.split())
                
            except Exception as e:
                logger.error(f"‚ùå Erro na an√°lise textual de {source}: {e}")
                continue

        # An√°lise agregada
        if all_entities:
            entity_counter = Counter(all_entities)
            results["entities_found"] = {
                str(entity): count for entity, count in entity_counter.most_common(50)
            }

        # Extra√ß√£o de t√≥picos com LDA
        if HAS_SKLEARN and HAS_GENSIM and all_texts:
            try:
                topics = self._extract_topics_lda(all_texts)
                results["key_topics"] = topics
                
                # Clustering sem√¢ntico
                clusters = self._perform_semantic_clustering(all_texts)
                results["semantic_clusters"] = clusters
                
            except Exception as e:
                logger.error(f"‚ùå Erro na extra√ß√£o de t√≥picos: {e}")

        # Densidade de palavras-chave
        if all_texts:
            keyword_density = self._calculate_keyword_density(all_texts)
            results["keyword_density"] = keyword_density

        # Temas emergentes
        emerging_themes = self._identify_emerging_themes(all_texts)
        results["emerging_themes"] = emerging_themes

        logger.info("‚úÖ An√°lise textual ultra-profunda conclu√≠da")
        return results

    async def _perform_temporal_analysis(self, session_dir: Path) -> Dict[str, Any]:
        """Analisa tend√™ncias temporais e padr√µes de crescimento"""
        
        results = {
            "data_points_analyzed": 0,
            "growth_rates": {},
            "seasonality_patterns": {},
            "velocity_of_change": {},
            "trend_acceleration": {},
            "cyclical_patterns": {},
            "anomaly_detection": {},
            "forecast_models": {}
        }

        # Carrega dados com timestamps
        temporal_data = self._gather_temporal_data(session_dir)
        
        if not temporal_data:
            logger.warning("‚ö†Ô∏è Dados temporais insuficientes para an√°lise")
            return results

        results["data_points_analyzed"] = len(temporal_data)

        try:
            # Converte para DataFrame para an√°lise
            df = pd.DataFrame(temporal_data)
            
            if 'timestamp' in df.columns and len(df) >= self.config['min_data_points_prediction']:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.sort_values('timestamp')
                
                # An√°lise de crescimento
                growth_analysis = self._analyze_growth_patterns(df)
                results["growth_rates"] = growth_analysis
                
                # Detec√ß√£o de sazonalidade
                if len(df) >= 10:  # M√≠nimo para an√°lise sazonal
                    seasonality = self._detect_seasonality(df)
                    results["seasonality_patterns"] = seasonality
                
                # Velocidade de mudan√ßa
                velocity = self._calculate_velocity_of_change(df)
                results["velocity_of_change"] = velocity
                
                # Acelera√ß√£o de tend√™ncias
                acceleration = self._calculate_trend_acceleration(df)
                results["trend_acceleration"] = acceleration
                
                # Detec√ß√£o de anomalias
                anomalies = self._detect_anomalies(df)
                results["anomaly_detection"] = anomalies
                
                # Modelos de previs√£o
                if HAS_PROPHET and len(df) >= 10:
                    forecast = self._create_forecast_models(df)
                    results["forecast_models"] = forecast

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise temporal: {e}")

        logger.info("‚úÖ An√°lise temporal conclu√≠da")
        return results

    async def _perform_advanced_visual_analysis(self, session_dir: Path) -> Dict[str, Any]:
        """Realiza an√°lise visual avan√ßada com OCR e Computer Vision"""
        
        results = {
            "screenshots_processed": 0,
            "text_extracted_ocr": [],
            "visual_elements_detected": {},
            "color_analysis": {},
            "layout_patterns": {},
            "ui_elements_identified": {},
            "brand_elements": {},
            "emotional_visual_cues": {},
            "accessibility_metrics": {}
        }

        if not HAS_OCR:
            logger.warning("‚ö†Ô∏è OCR n√£o dispon√≠vel - an√°lise visual limitada")
            return results

        files_dir = Path(f"analyses_data/files/{session_id}")
        if not files_dir.exists():
            logger.info("üìÇ Diret√≥rio de screenshots n√£o encontrado")
            return results

        extracted_texts = []
        visual_features = []

        for img_file in files_dir.glob("*.png"):
            try:
                logger.info(f"üîç Analisando imagem: {img_file.name}")
                
                # Carrega imagem
                image = Image.open(img_file)
                
                # OCR para extra√ß√£o de texto
                ocr_text = pytesseract.image_to_string(image, lang='por')
                if ocr_text.strip():
                    extracted_texts.append(ocr_text)
                    results["text_extracted_ocr"].append({
                        "file": img_file.name,
                        "text": ocr_text[:500],  # Limita para armazenamento
                        "word_count": len(ocr_text.split())
                    })
                
                # An√°lise de cores (se OpenCV dispon√≠vel)
                if HAS_OPENCV:
                    color_analysis = self._analyze_image_colors(img_file)
                    results["color_analysis"][img_file.name] = color_analysis
                
                # An√°lise de layout e elementos UI
                ui_elements = self._detect_ui_elements(ocr_text)
                results["ui_elements_identified"][img_file.name] = ui_elements
                
                # Elementos de marca
                brand_elements = self._detect_brand_elements(ocr_text)
                results["brand_elements"][img_file.name] = brand_elements
                
                # Indicadores emocionais visuais
                emotional_cues = self._extract_visual_emotional_cues(ocr_text)
                results["emotional_visual_cues"][img_file.name] = emotional_cues
                
                results["screenshots_processed"] += 1
                
            except Exception as e:
                logger.error(f"‚ùå Erro na an√°lise visual de {img_file.name}: {e}")
                continue

        # An√°lise agregada do texto extra√≠do
        if extracted_texts:
            combined_text = " ".join(extracted_texts)
            
            # Palavras-chave visuais
            visual_keywords = self._extract_visual_keywords(combined_text)
            results["visual_keywords"] = visual_keywords
            
            # Padr√µes de layout
            layout_patterns = self._identify_layout_patterns(extracted_texts)
            results["layout_patterns"] = layout_patterns

        logger.info(f"‚úÖ An√°lise visual conclu√≠da: {results['screenshots_processed']} imagens processadas")
        return results

    async def _perform_network_analysis(self, session_dir: Path) -> Dict[str, Any]:
        """Realiza an√°lise de rede e conectividade entre entidades"""
        
        results = {
            "network_nodes": 0,
            "network_edges": 0,
            "centrality_metrics": {},
            "community_detection": {},
            "influence_paths": {},
            "network_density": 0,
            "clustering_coefficient": 0,
            "small_world_metrics": {}
        }

        if not HAS_NETWORKX:
            logger.warning("‚ö†Ô∏è NetworkX n√£o dispon√≠vel - an√°lise de rede desabilitada")
            return results

        try:
            # Carrega dados de entidades e relacionamentos
            entities_data = self._extract_entities_relationships(session_dir)
            
            if not entities_data:
                logger.warning("‚ö†Ô∏è Dados insuficientes para an√°lise de rede")
                return results

            # Cria grafo
            G = nx.Graph()
            
            # Adiciona n√≥s (entidades)
            for entity in entities_data['entities']:
                G.add_node(entity['name'], **entity['attributes'])
            
            # Adiciona arestas (relacionamentos)
            for relationship in entities_data['relationships']:
                G.add_edge(
                    relationship['source'], 
                    relationship['target'], 
                    weight=relationship['strength']
                )

            results["network_nodes"] = G.number_of_nodes()
            results["network_edges"] = G.number_of_edges()
            results["network_density"] = nx.density(G)

            # M√©tricas de centralidade
            if G.number_of_nodes() > 0:
                centrality = {
                    "betweenness": dict(nx.betweenness_centrality(G)),
                    "closeness": dict(nx.closeness_centrality(G)),
                    "degree": dict(nx.degree_centrality(G)),
                    "eigenvector": dict(nx.eigenvector_centrality(G, max_iter=1000))
                }
                results["centrality_metrics"] = centrality
                
                # Detec√ß√£o de comunidades
                communities = list(nx.community.greedy_modularity_communities(G))
                results["community_detection"] = {
                    "num_communities": len(communities),
                    "modularity": nx.community.modularity(G, communities),
                    "communities": [list(community) for community in communities]
                }
                
                # Coeficiente de clustering
                results["clustering_coefficient"] = nx.average_clustering(G)

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de rede: {e}")

        logger.info("‚úÖ An√°lise de rede conclu√≠da")
        return results

    async def _analyze_sentiment_dynamics(self, session_dir: Path) -> Dict[str, Any]:
        """Analisa din√¢mica e evolu√ß√£o de sentimentos"""
        
        results = {
            "overall_sentiment_trend": {},
            "sentiment_volatility": {},
            "emotional_peaks": [],
            "sentiment_drivers": {},
            "mood_transitions": {},
            "sentiment_correlation": {},
            "emotional_contagion": {}
        }

        if not HAS_VADER:
            logger.warning("‚ö†Ô∏è Analisador de sentimento n√£o dispon√≠vel")
            return results

        try:
            # Carrega dados com sentimentos
            sentiment_data = self._gather_sentiment_data(session_dir)
            
            if not sentiment_data:
                logger.warning("‚ö†Ô∏è Dados insuficientes para an√°lise de sentimento")
                return results

            # An√°lise de tend√™ncia geral
            overall_sentiment = self._calculate_overall_sentiment_trend(sentiment_data)
            results["overall_sentiment_trend"] = overall_sentiment
            
            # Volatilidade de sentimento
            volatility = self._calculate_sentiment_volatility(sentiment_data)
            results["sentiment_volatility"] = volatility
            
            # Picos emocionais
            peaks = self._identify_emotional_peaks(sentiment_data)
            results["emotional_peaks"] = peaks
            
            # Drivers de sentimento
            drivers = self._identify_sentiment_drivers(sentiment_data)
            results["sentiment_drivers"] = drivers

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de sentimento: {e}")

        logger.info("‚úÖ An√°lise de din√¢mica de sentimentos conclu√≠da")
        return results

    async def _analyze_topic_evolution(self, session_dir: Path) -> Dict[str, Any]:
        """Analisa evolu√ß√£o e mudan√ßa de t√≥picos ao longo do tempo"""
        
        results = {
            "topic_lifecycle": {},
            "emerging_topics": [],
            "declining_topics": [],
            "stable_topics": [],
            "topic_transitions": {},
            "topic_velocity": {},
            "topic_influence_network": {}
        }

        try:
            # Carrega dados temporais de t√≥picos
            topic_data = self._gather_topic_temporal_data(session_dir)
            
            if not topic_data:
                logger.warning("‚ö†Ô∏è Dados insuficientes para an√°lise de evolu√ß√£o de t√≥picos")
                return results

            # An√°lise de ciclo de vida dos t√≥picos
            lifecycle = self._analyze_topic_lifecycle(topic_data)
            results["topic_lifecycle"] = lifecycle
            
            # Identifica√ß√£o de t√≥picos emergentes vs em decl√≠nio
            emerging, declining, stable = self._classify_topic_trends(topic_data)
            results["emerging_topics"] = emerging
            results["declining_topics"] = declining
            results["stable_topics"] = stable
            
            # Transi√ß√µes entre t√≥picos
            transitions = self._analyze_topic_transitions(topic_data)
            results["topic_transitions"] = transitions

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de evolu√ß√£o de t√≥picos: {e}")

        logger.info("‚úÖ An√°lise de evolu√ß√£o de t√≥picos conclu√≠da")
        return results

    async def _analyze_engagement_patterns(self, session_dir: Path) -> Dict[str, Any]:
        """Analisa padr√µes de engajamento e intera√ß√£o"""
        
        results = {
            "engagement_metrics": {},
            "viral_patterns": {},
            "audience_behavior": {},
            "content_performance": {},
            "engagement_drivers": {},
            "optimal_timing": {},
            "platform_preferences": {}
        }

        try:
            # Carrega dados de engajamento
            engagement_data = self._gather_engagement_data(session_dir)
            
            if not engagement_data:
                logger.warning("‚ö†Ô∏è Dados de engajamento insuficientes")
                return results

            # M√©tricas de engajamento
            metrics = self._calculate_engagement_metrics(engagement_data)
            results["engagement_metrics"] = metrics
            
            # Padr√µes virais
            viral_patterns = self._identify_viral_patterns(engagement_data)
            results["viral_patterns"] = viral_patterns
            
            # Comportamento da audi√™ncia
            audience_behavior = self._analyze_audience_behavior(engagement_data)
            results["audience_behavior"] = audience_behavior
            
            # Performance de conte√∫do
            content_performance = self._analyze_content_performance(engagement_data)
            results["content_performance"] = content_performance

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de padr√µes de engajamento: {e}")

        logger.info("‚úÖ An√°lise de padr√µes de engajamento conclu√≠da")
        return results

    async def _generate_ultra_predictions(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Gera previs√µes ultra-avan√ßadas baseadas em todos os insights"""
        
        predictions = {
            "market_growth_forecast": {},
            "trend_predictions": {},
            "sentiment_forecast": {},
            "engagement_predictions": {},
            "competitive_landscape_evolution": {},
            "technology_adoption_curve": {},
            "consumer_behavior_shifts": {},
            "risk_probability_matrix": {},
            "opportunity_timeline": {},
            "strategic_inflection_points": {}
        }

        try:
            # Previs√£o de crescimento de mercado
            market_forecast = self._predict_market_growth(insights)
            predictions["market_growth_forecast"] = market_forecast
            
            # Previs√£o de tend√™ncias
            trend_predictions = self._predict_trend_evolution(insights)
            predictions["trend_predictions"] = trend_predictions
            
            # Previs√£o de sentimento
            sentiment_forecast = self._predict_sentiment_evolution(insights)
            predictions["sentiment_forecast"] = sentiment_forecast
            
            # Previs√£o de engajamento
            engagement_predictions = self._predict_engagement_patterns(insights)
            predictions["engagement_predictions"] = engagement_predictions
            
            # Evolu√ß√£o do cen√°rio competitivo
            competitive_evolution = self._predict_competitive_evolution(insights)
            predictions["competitive_landscape_evolution"] = competitive_evolution
            
            # Curva de ado√ß√£o tecnol√≥gica
            adoption_curve = self._model_technology_adoption(insights)
            predictions["technology_adoption_curve"] = adoption_curve
            
            # Mudan√ßas comportamentais do consumidor
            behavior_shifts = self._predict_consumer_behavior_shifts(insights)
            predictions["consumer_behavior_shifts"] = behavior_shifts
            
            # Matriz de probabilidade de riscos
            risk_matrix = self._create_risk_probability_matrix(insights)
            predictions["risk_probability_matrix"] = risk_matrix
            
            # Timeline de oportunidades
            opportunity_timeline = self._create_opportunity_timeline(insights)
            predictions["opportunity_timeline"] = opportunity_timeline
            
            # Pontos de inflex√£o estrat√©gica
            inflection_points = self._identify_strategic_inflection_points(insights)
            predictions["strategic_inflection_points"] = inflection_points

        except Exception as e:
            logger.error(f"‚ùå Erro na gera√ß√£o de previs√µes: {e}")

        logger.info("‚úÖ Previs√µes ultra-avan√ßadas geradas")
        return predictions

    async def _model_complex_scenarios(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela cen√°rios complexos e multidimensionais"""
        
        scenarios = {
            "base_scenario": {},
            "optimistic_scenario": {},
            "pessimistic_scenario": {},
            "disruptive_scenario": {},
            "regulatory_change_scenario": {},
            "economic_crisis_scenario": {},
            "technology_breakthrough_scenario": {},
            "competitive_disruption_scenario": {},
            "scenario_probabilities": {},
            "scenario_impact_matrix": {},
            "contingency_plans": {}
        }

        try:
            # Cen√°rio base (mais prov√°vel)
            base_scenario = self._model_base_scenario(insights)
            scenarios["base_scenario"] = base_scenario
            
            # Cen√°rio otimista
            optimistic_scenario = self._model_optimistic_scenario(insights)
            scenarios["optimistic_scenario"] = optimistic_scenario
            
            # Cen√°rio pessimista
            pessimistic_scenario = self._model_pessimistic_scenario(insights)
            scenarios["pessimistic_scenario"] = pessimistic_scenario
            
            # Cen√°rio disruptivo
            disruptive_scenario = self._model_disruptive_scenario(insights)
            scenarios["disruptive_scenario"] = disruptive_scenario
            
            # Cen√°rios espec√≠ficos
            regulatory_scenario = self._model_regulatory_change_scenario(insights)
            scenarios["regulatory_change_scenario"] = regulatory_scenario
            
            economic_scenario = self._model_economic_crisis_scenario(insights)
            scenarios["economic_crisis_scenario"] = economic_scenario
            
            tech_scenario = self._model_technology_breakthrough_scenario(insights)
            scenarios["technology_breakthrough_scenario"] = tech_scenario
            
            competitive_scenario = self._model_competitive_disruption_scenario(insights)
            scenarios["competitive_disruption_scenario"] = competitive_scenario
            
            # Probabilidades dos cen√°rios
            probabilities = self._calculate_scenario_probabilities(insights)
            scenarios["scenario_probabilities"] = probabilities
            
            # Matriz de impacto
            impact_matrix = self._create_scenario_impact_matrix(scenarios)
            scenarios["scenario_impact_matrix"] = impact_matrix
            
            # Planos de conting√™ncia
            contingency_plans = self._generate_contingency_plans(scenarios)
            scenarios["contingency_plans"] = contingency_plans

        except Exception as e:
            logger.error(f"‚ùå Erro na modelagem de cen√°rios: {e}")

        logger.info("‚úÖ Modelagem de cen√°rios complexos conclu√≠da")
        return scenarios

    # M√©todos auxiliares para an√°lise textual
    def _gather_comprehensive_textual_data(self, session_dir: Path) -> Dict[str, str]:
        """Coleta dados textuais de arquivos na pasta da sess√£o."""
        textual_data = {}
        text_files = [f for f in session_dir.glob("*.txt")]
        for text_file in text_files:
            try:
                with open(text_file, "r", encoding="utf-8") as f:
                    textual_data[text_file.name] = f.read()
            except Exception as e:
                logger.error(f"‚ùå Erro ao ler arquivo de texto {text_file.name}: {e}")
        return textual_data

    def _extract_topics_lda(self, texts: List[str]) -> List[Dict[str, Any]]:
        """Extrai t√≥picos de um conjunto de textos usando LDA."""
        if not HAS_GENSIM or not HAS_SKLEARN:
            logger.warning("‚ö†Ô∏è Gensim ou Scikit-learn n√£o dispon√≠veis para extra√ß√£o de t√≥picos LDA.")
            return []

        try:
            # Pr√©-processamento para Gensim
            processed_texts = [[word for word in doc.lower().split() if word.isalpha() and word not in self._get_portuguese_stopwords()] for doc in texts]
            dictionary = corpora.Dictionary(processed_texts)
            corpus = [dictionary.doc2bow(text) for text in processed_texts]
            
            # Treina o modelo LDA
            lda_model = models.LdaMulticore(corpus, num_topics=self.config["n_topics_lda"], id2word=dictionary, passes=10, workers=2)
            self.topic_model = lda_model # Armazena o modelo treinado

            topics = []
            
            for idx, topic in lda_model.print_topics(-1):
                topics.append({
                    "topic_id": idx,
                    "words": topic,
                    "weight": 1.0 / (idx + 1)  # Peso decrescente
                })
 
            return topics
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair t√≥picos com LDA: {e}")
            return []

    def _perform_semantic_clustering(self, texts: List[str]) -> Dict[str, Any]:
        """Realiza clustering sem√¢ntico de textos usando TF-IDF e KMeans."""
        if not HAS_SKLEARN:
            logger.warning("‚ö†Ô∏è Scikit-learn n√£o dispon√≠vel para clustering sem√¢ntico.")
            return {}

        try:
            # Transforma os textos em vetores TF-IDF
            X = self.tfidf_vectorizer.fit_transform(texts)

            # Aplica KMeans
            kmeans_model = KMeans(n_clusters=self.config["n_clusters_kmeans"], init='k-means++', max_iter=300, random_state=42, n_init=10)
            kmeans_model.fit(X)
            
            clusters = defaultdict(list)
            for i, label in enumerate(kmeans_model.labels_):
                clusters[f"cluster_{label}"].append(texts[i])
            
            # Extrai as palavras-chave para cada cluster
            order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]
            terms = self.tfidf_vectorizer.get_feature_names_out()
            
            cluster_keywords = {}
            for i in range(self.config["n_clusters_kmeans"]):
                cluster_keywords[f"cluster_{i}"] = [terms[ind] for ind in order_centroids[i, :10]]

            return {"clusters": {k: v for k, v in clusters.items()}, "cluster_keywords": cluster_keywords}
        except Exception as e:
            logger.error(f"‚ùå Erro no clustering sem√¢ntico: {e}")
            return {}




    def _calculate_keyword_density(self, texts: List[str]) -> Dict[str, float]:
        """Calcula a densidade de palavras-chave em um conjunto de textos."""
        if not texts:
            return {}

        combined_text = " ".join(texts).lower()
        words = [word for word in re.findall(r'\b\w+\b', combined_text) if word not in self._get_portuguese_stopwords()]
        word_counts = Counter(words)
        total_words = len(words)

        if total_words == 0:
            return {}

        density = {word: (count / total_words) * 100 for word, count in word_counts.most_common(50)}
        return density




    def _identify_emerging_themes(self, texts: List[str]) -> List[str]:
        """Identifica temas emergentes analisando a frequ√™ncia e co-ocorr√™ncia de termos."""
        if not texts:
            return []

        # Para simplificar, usaremos uma abordagem baseada em frequ√™ncia e n-grams
        # Uma abordagem mais avan√ßada envolveria an√°lise temporal de t√≥picos ou detec√ß√£o de anomalias em termos.
        
        all_words = []
        for text in texts:
            words = [word for word in re.findall(r'\b\w+\b', text.lower()) if word not in self._get_portuguese_stopwords()]
            all_words.extend(words)

        word_freq = Counter(all_words)
        
        # Considerar palavras que apareceram recentemente ou tiveram um aumento significativo
        # Esta √© uma simula√ß√£o, pois n√£o temos dados temporais aqui. Em um cen√°rio real, precisar√≠amos de timestamps.
        # Para este exemplo, vamos pegar as 20 palavras mais frequentes como 'temas emergentes' simplificados.
        emerging_themes = [word for word, freq in word_freq.most_common(20)]
        
        return emerging_themes




    def _gather_temporal_data(self, session_dir: Path) -> List[Dict[str, Any]]:
        """Simula a coleta de dados temporais de arquivos na sess√£o."""
        temporal_data = []
        # Exemplo: busca por arquivos JSON que contenham dados com timestamps
        # Em um cen√°rio real, isso leria dados de logs, eventos, etc.
        for f in session_dir.glob("*.json"):
            try:
                with open(f, 'r', encoding='utf-8') as infile:
                    data = json.load(infile)
                    if isinstance(data, list):
                        for item in data:
                            if "timestamp" in item and "value" in item:
                                try:
                                    item["timestamp"] = datetime.fromisoformat(item["timestamp"])
                                    temporal_data.append(item)
                                except ValueError:
                                    continue
                    elif isinstance(data, dict):
                        if "timestamp" in data and "value" in data:
                            try:
                                data["timestamp"] = datetime.fromisoformat(data["timestamp"])
                                temporal_data.append(data)
                            except ValueError:
                                continue
            except json.JSONDecodeError:
                continue
        
        # Ordena os dados por timestamp
        temporal_data.sort(key=lambda x: x["timestamp"])
        return temporal_data




    def _analyze_growth_patterns(self, temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa padr√µes de crescimento em dados temporais."""
        if not temporal_data:
            return {}

        df = pd.DataFrame(temporal_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        growth_patterns = {}
        # Exemplo: c√°lculo de crescimento di√°rio, semanal, mensal
        # Isso pode ser expandido para diferentes granularidades e m√©tricas
        if "value" in df.columns:
            # Crescimento di√°rio
            daily_growth = df["value"].diff().mean()
            growth_patterns["daily_average_growth"] = daily_growth

            # Crescimento percentual mensal (exemplo simplificado)
            monthly_resampled = df["value"].resample("M").last()
            if len(monthly_resampled) > 1:
                monthly_growth_rate = (monthly_resampled.iloc[-1] - monthly_resampled.iloc[-2]) / monthly_resampled.iloc[-2]
                growth_patterns["monthly_growth_rate"] = monthly_growth_rate

        return growth_patterns




    def _detect_seasonality(self, temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detecta padr√µes de sazonalidade em dados temporais."""
        if not temporal_data:
            return {}

        df = pd.DataFrame(temporal_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        seasonality_patterns = {}

        if "value" in df.columns and len(df) > 2 * 7: # M√≠nimo de duas semanas para detectar sazonalidade semanal
            # Exemplo: Sazonalidade semanal (m√©dia por dia da semana)
            df["day_of_week"] = df.index.dayofweek
            weekly_seasonality = df.groupby("day_of_week")["value"].mean().to_dict()
            seasonality_patterns["weekly_seasonality"] = weekly_seasonality

            # Exemplo: Sazonalidade mensal (m√©dia por m√™s)
            df["month"] = df.index.month
            monthly_seasonality = df.groupby("month")["value"].mean().to_dict()
            seasonality_patterns["monthly_seasonality"] = monthly_seasonality

        return seasonality_patterns




    def _calculate_velocity_of_change(self, temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calcula a velocidade de mudan√ßa de uma m√©trica ao longo do tempo."""
        if not temporal_data or len(temporal_data) < 2:
            return {}

        df = pd.DataFrame(temporal_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        velocity_of_change = {}
        if "value" in df.columns:
            # Calcula a primeira derivada (taxa de mudan√ßa)
            df["change"] = df["value"].diff()
            velocity_of_change["average_change_per_period"] = df["change"].mean()
            velocity_of_change["max_change_per_period"] = df["change"].max()
            velocity_of_change["min_change_per_period"] = df["change"].min()

        return velocity_of_change




    def _calculate_trend_acceleration(self, temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calcula a acelera√ß√£o da tend√™ncia (segunda derivada)."""
        if not temporal_data or len(temporal_data) < 3:
            return {}

        df = pd.DataFrame(temporal_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        trend_acceleration = {}
        if "value" in df.columns:
            # Calcula a primeira derivada (velocidade)
            df["velocity"] = df["value"].diff()
            # Calcula a segunda derivada (acelera√ß√£o)
            df["acceleration"] = df["velocity"].diff()
            trend_acceleration["average_acceleration"] = df["acceleration"].mean()
            trend_acceleration["max_acceleration"] = df["acceleration"].max()
            trend_acceleration["min_acceleration"] = df["acceleration"].min()

        return trend_acceleration




    def _detect_anomalies(self, temporal_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detecta anomalias em dados temporais usando um m√©todo simples (e.g., IQR)."""
        if not temporal_data or len(temporal_data) < 5:
            return []

        df = pd.DataFrame(temporal_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        anomalies = []
        if "value" in df.columns:
            Q1 = df["value"].quantile(0.25)
            Q3 = df["value"].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            for index, row in df.iterrows():
                if row["value"] < lower_bound or row["value"] > upper_bound:
                    anomalies.append({"timestamp": index.isoformat(), "value": row["value"], "type": "outlier"})

        return anomalies




    def _create_forecast_models(self, temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Cria modelos de previs√£o usando Prophet (se dispon√≠vel) ou regress√£o linear."""
        forecast_models = {}
        if not temporal_data or len(temporal_data) < self.config["min_data_points_prediction"]:
            logger.warning("‚ö†Ô∏è Dados insuficientes para criar modelos de previs√£o.")
            return forecast_models

        df = pd.DataFrame(temporal_data)
        df["ds"] = pd.to_datetime(df["timestamp"])
        df["y"] = df["value"]

        if HAS_PROPHET:
            try:
                m = Prophet()
                m.fit(df)
                future = m.make_future_dataframe(periods=self.config["prediction_horizon_days"])
                forecast = m.predict(future)
                forecast_models["prophet_forecast"] = forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]].to_dict(orient="records")
                logger.info("‚úÖ Modelo Prophet criado e previs√£o gerada.")
            except Exception as e:
                logger.error(f"‚ùå Erro ao criar modelo Prophet: {e}")
        
        if HAS_SKLEARN:
            try:
                # Regress√£o Linear como fallback ou modelo adicional
                df["ordinal_date"] = df["ds"].apply(lambda date: date.toordinal())
                X = df[["ordinal_date"]]
                y = df["y"]

                if len(X) > 1:
                    model = LinearRegression()
                    model.fit(X, y)

                    # Prever para o futuro
                    last_date_ordinal = df["ordinal_date"].max()
                    future_dates_ordinal = np.array([last_date_ordinal + i for i in range(1, self.config["prediction_horizon_days"] + 1)]).reshape(-1, 1)
                    future_predictions = model.predict(future_dates_ordinal)
                    
                    forecast_models["linear_regression_forecast"] = [
                        {"ds": datetime.fromordinal(int(d)).isoformat(), "yhat": p}
                        for d, p in zip(future_dates_ordinal.flatten(), future_predictions)
                    ]
                    logger.info("‚úÖ Modelo de Regress√£o Linear criado e previs√£o gerada.")
            except Exception as e:
                logger.error(f"‚ùå Erro ao criar modelo de Regress√£o Linear: {e}")

        return forecast_models




    def _analyze_image_colors(self, image_path: Path) -> Dict[str, Any]:
        """Analisa as cores predominantes em uma imagem."""
        if not HAS_OPENCV:
            logger.warning("‚ö†Ô∏è OpenCV n√£o dispon√≠vel para an√°lise de cores de imagem.")
            return {}

        try:
            img = cv2.imread(str(image_path))
            if img is None:
                logger.error(f"‚ùå N√£o foi poss√≠vel carregar a imagem: {image_path}")
                return {}

            # Redimensiona a imagem para acelerar o processamento
            img = cv2.resize(img, (100, 100))
            
            # Converte para RGB (OpenCV l√™ em BGR)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Remodela a imagem para uma lista de pixels
            pixels = img.reshape((-1, 3))
            pixels = np.float32(pixels)

            # Define crit√©rios de parada e aplica KMeans para encontrar as cores predominantes
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)
            k = 5  # N√∫mero de cores predominantes a serem encontradas
            _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

            # Converte os centros de volta para inteiros de 8 bits
            centers = np.uint8(centers)

            # Conta a frequ√™ncia de cada cor
            counts = Counter(labels.flatten())
            
            # Ordena as cores pela frequ√™ncia
            sorted_colors = sorted(counts.items(), key=lambda x: x[1], reverse=True)

            dominant_colors = []
            for i, count in sorted_colors:
                color = centers[i]
                dominant_colors.append({"rgb": color.tolist(), "percentage": (count / len(pixels)) * 100})

            return {"dominant_colors": dominant_colors}
        except Exception as e:
            logger.error(f"‚ùå Erro ao analisar cores da imagem {image_path}: {e}")
            return {}




    def _detect_ui_elements(self, text_content: str) -> Dict[str, Any]:
        """Detecta elementos de UI em texto extra√≠do de imagens (OCR)."""
        # Esta √© uma implementa√ß√£o simplificada baseada em padr√µes de texto.
        # Uma solu√ß√£o mais robusta exigiria modelos de Computer Vision treinados para detec√ß√£o de UI.
        ui_elements = defaultdict(int)

        # Exemplos de detec√ß√£o de elementos comuns de UI por palavras-chave
        if re.search(r'bot√£o|clique aqui|submit|enviar', text_content, re.IGNORECASE):
            ui_elements["button"] += 1
        if re.search(r'campo de texto|digite aqui|input|pesquisar', text_content, re.IGNORECASE):
            ui_elements["text_input"] += 1
        if re.search(r'menu|navega√ß√£o|sidebar', text_content, re.IGNORECASE):
            ui_elements["navigation_menu"] += 1
        if re.search(r'cabe√ßalho|header', text_content, re.IGNORECASE):
            ui_elements["header"] += 1
        if re.search(r'rodap√©|footer', text_content, re.IGNORECASE):
            ui_elements["footer"] += 1
        if re.search(r'imagem|foto|galeria', text_content, re.IGNORECASE):
            ui_elements["image_element"] += 1
        if re.search(r'v√≠deo|player', text_content, re.IGNORECASE):
            ui_elements["video_element"] += 1
        if re.search(r'checkbox|radio button', text_content, re.IGNORECASE):
            ui_elements["form_control"] += 1

        return dict(ui_elements)




    def _detect_brand_elements(self, text_content: str) -> Dict[str, Any]:
        """Detecta elementos de marca em texto extra√≠do de imagens (OCR)."""
        # Esta √© uma implementa√ß√£o simplificada baseada em padr√µes de texto.
        # Uma solu√ß√£o mais robusta exigiria uma base de dados de logos, cores de marca, fontes, etc.
        brand_elements = defaultdict(int)

        # Exemplos de detec√ß√£o de elementos de marca por palavras-chave
        if re.search(r'logo|logotipo|marca registrada|slogan', text_content, re.IGNORECASE):
            brand_elements["logo_slogan_mention"] += 1
        if re.search(r'nome da empresa|nome da marca', text_content, re.IGNORECASE):
            brand_elements["company_name_mention"] += 1
        if re.search(r'direitos autorais|copyright', text_content, re.IGNORECASE):
            brand_elements["copyright_mention"] += 1
        if re.search(r'site oficial|www\.|\.com|\.br', text_content, re.IGNORECASE):
            brand_elements["website_mention"] += 1
        if re.search(r'registrado|¬Æ|‚Ñ¢', text_content, re.IGNORECASE):
            brand_elements["trademark_symbol"] += 1

        # Poderia ser expandido para detec√ß√£o de cores espec√≠ficas (se a an√°lise de cores for integrada)
        # ou reconhecimento de fontes (mais complexo via OCR)

        return dict(brand_elements)




    def _extract_visual_emotional_cues(self, text_content: str) -> Dict[str, Any]:
        """Extrai indicadores emocionais visuais de texto (simulado via OCR)."""
        # Esta fun√ß√£o simula a extra√ß√£o de pistas emocionais de conte√∫do visual
        # atrav√©s do texto extra√≠do por OCR. Em um cen√°rio real, isso envolveria
        # modelos de Computer Vision para an√°lise de express√µes faciais, cenas, etc.
        emotional_cues = defaultdict(int)

        # Palavras-chave associadas a emo√ß√µes positivas
        if re.search(r'feliz|alegre|sorriso|sucesso|√≥timo|bom|excelente|positivo', text_content, re.IGNORECASE):
            emotional_cues["positive_emotion_keywords"] += 1
        # Palavras-chave associadas a emo√ß√µes negativas
        if re.search(r'triste|bravo|raiva|preocupado|problema|ruim|negativo', text_content, re.IGNORECASE):
            emotional_cues["negative_emotion_keywords"] += 1
        # Palavras-chave associadas a surpresa
        if re.search(r'surpresa|chocado|inesperado', text_content, re.IGNORECASE):
            emotional_cues["surprise_emotion_keywords"] += 1
        # Palavras-chave associadas a confian√ßa/seguran√ßa
        if re.search(r'confian√ßa|seguran√ßa|garantia|est√°vel', text_content, re.IGNORECASE):
            emotional_cues["trust_security_keywords"] += 1

        # Pode-se integrar com an√°lise de sentimento do texto para refor√ßar
        if HAS_VADER and self.sentiment_analyzer:
            sentiment = self.sentiment_analyzer.polarity_scores(text_content)
            if sentiment["compound"] > 0.05:
                emotional_cues["overall_positive_sentiment"] += 1
            elif sentiment["compound"] < -0.05:
                emotional_cues["overall_negative_sentiment"] += 1

        return dict(emotional_cues)




    def _extract_visual_keywords(self, combined_text: str) -> List[str]:
        """Extrai palavras-chave visuais do texto combinado de OCR."""
        if not combined_text:
            return []

        # Reutiliza a l√≥gica de densidade de palavras-chave ou t√≥picos para extrair palavras-chave relevantes
        # Aqui, uma abordagem simplificada √© pegar as palavras mais frequentes ap√≥s remover stopwords.
        words = [word for word in re.findall(r'\b\w+\b', combined_text.lower()) if word not in self._get_portuguese_stopwords()]
        word_counts = Counter(words)
        
        # Retorna as 20 palavras mais comuns como palavras-chave visuais
        visual_keywords = [word for word, count in word_counts.most_common(20)]
        return visual_keywords




    def _identify_layout_patterns(self, extracted_texts: List[str]) -> Dict[str, Any]:
        """Identifica padr√µes de layout com base no texto extra√≠do e sua estrutura."""
        # Esta fun√ß√£o √© uma simula√ß√£o. A detec√ß√£o real de padr√µes de layout
        # exigiria an√°lise espacial dos elementos visuais (bounding boxes do OCR).
        # Aqui, inferimos padr√µes de layout com base na presen√ßa de certos elementos textuais.
        layout_patterns = defaultdict(int)

        for text_content in extracted_texts:
            # Detec√ß√£o de cabe√ßalhos e rodap√©s (simples)
            if re.search(r'\n\s*\d{1,2}\s*\n', text_content) or re.search(r'\n\s*P√°gina\s*\d+\s*\n', text_content, re.IGNORECASE):
                layout_patterns["has_page_numbers"] += 1
            if re.search(r'\n\s*Copyright|Todos os direitos reservados\s*\n', text_content, re.IGNORECASE):
                layout_patterns["has_copyright_info"] += 1
            
            # Detec√ß√£o de listas (simples)
            if re.search(r'\n\s*[-*‚Ä¢]\s+\w+', text_content):
                layout_patterns["has_lists"] += 1
            
            # Detec√ß√£o de colunas (muito simplificado, apenas por indica√ß√£o de quebra de linha)
            # Uma detec√ß√£o real exigiria an√°lise de coordenadas X do texto.
            if re.search(r'\n\s*\S.{50,}\S\n\s*\S.{50,}\S', text_content):
                layout_patterns["has_multi_column_like_text"] += 1

            # Detec√ß√£o de blocos de texto grandes (par√°grafos)
            if len(text_content) > 500:
                layout_patterns["has_large_text_blocks"] += 1

        return dict(layout_patterns)




    def _extract_entities_relationships(self, session_dir: Path) -> Dict[str, Any]:
        """Extrai entidades e relacionamentos de dados textuais na sess√£o."""
        entities = []
        relationships = []

        # Simula a leitura de dados textuais para extra√ß√£o de entidades
        textual_data = self._gather_comprehensive_textual_data(session_dir)

        if not HAS_SPACY or not self.nlp_model:
            logger.warning("‚ö†Ô∏è SpaCy n√£o dispon√≠vel para extra√ß√£o de entidades e relacionamentos.")
            return {"entities": entities, "relationships": relationships}

        for source, text_content in textual_data.items():
            try:
                doc = self.nlp_model(text_content[:1000000]) # Limita para performance
                
                # Extrai entidades
                for ent in doc.ents:
                    entities.append({"name": ent.text.strip(), "type": ent.label_, "source": source})
                
                # Extrai relacionamentos (simplificado: co-ocorr√™ncia de entidades na mesma frase)
                for sentence in doc.sents:
                    sentence_entities = [ent.text.strip() for ent in sentence.ents if ent.label_ in ["PERSON", "ORG", "GPE"]]
                    if len(sentence_entities) >= 2:
                        # Cria relacionamentos entre todas as pares de entidades na frase
                        for i in range(len(sentence_entities)):
                            for j in range(i + 1, len(sentence_entities)):
                                relationships.append({
                                    "source": sentence_entities[i],
                                    "target": sentence_entities[j],
                                    "type": "co-occurrence",
                                    "strength": 1.0 # Pode ser aprimorado com an√°lise de depend√™ncia
                                })
            except Exception as e:
                logger.error(f"‚ùå Erro ao extrair entidades/relacionamentos de {source}: {e}")
                continue

        return {"entities": entities, "relationships": relationships}




    def _gather_sentiment_data(self, session_dir: Path) -> List[Dict[str, Any]]:
        """Simula a coleta de dados de sentimento de arquivos na sess√£o."""
        sentiment_data = []
        # Exemplo: busca por arquivos JSON que contenham dados de sentimento com timestamps
        for f in session_dir.glob("*.json"):
            try:
                with open(f, 'r', encoding='utf-8') as infile:
                    data = json.load(infile)
                    if isinstance(data, list):
                        for item in data:
                            if "timestamp" in item and "sentiment_score" in item:
                                try:
                                    item["timestamp"] = datetime.fromisoformat(item["timestamp"])
                                    sentiment_data.append(item)
                                except ValueError:
                                    continue
                    elif isinstance(data, dict):
                        if "timestamp" in data and "sentiment_score" in data:
                            try:
                                data["timestamp"] = datetime.fromisoformat(data["timestamp"])
                                sentiment_data.append(data)
                            except ValueError:
                                continue
            except json.JSONDecodeError:
                continue
        
        # Ordena os dados por timestamp
        sentiment_data.sort(key=lambda x: x["timestamp"])
        return sentiment_data




    def _calculate_overall_sentiment_trend(self, sentiment_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calcula a tend√™ncia geral do sentimento ao longo do tempo."""
        if not sentiment_data:
            return {}

        df = pd.DataFrame(sentiment_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        overall_trend = {}
        if "sentiment_score" in df.columns:
            # M√©dia m√≥vel do sentimento
            overall_trend["moving_average_sentiment"] = df["sentiment_score"].rolling(window=7).mean().dropna().to_dict()
            # Tend√™ncia linear
            if len(df) > 1:
                df["ordinal_date"] = df.index.map(datetime.toordinal)
                model = LinearRegression()
                X = df[["ordinal_date"]]
                y = df["sentiment_score"]
                model.fit(X, y)
                overall_trend["linear_trend_slope"] = model.coef_[0]
                overall_trend["linear_trend_intercept"] = model.intercept_

        return overall_trend




    def _calculate_sentiment_volatility(self, sentiment_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calcula a volatilidade do sentimento ao longo do tempo."""
        if not sentiment_data:
            return {}

        df = pd.DataFrame(sentiment_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        volatility = {}
        if "sentiment_score" in df.columns:
            # Desvio padr√£o do sentimento em janelas m√≥veis
            volatility["rolling_std_dev_sentiment"] = df["sentiment_score"].rolling(window=7).std().dropna().to_dict()
            # Amplitude total do sentimento
            volatility["overall_range_sentiment"] = df["sentiment_score"].max() - df["sentiment_score"].min()

        return volatility




    def _identify_emotional_peaks(self, sentiment_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identifica picos emocionais (momentos de sentimento extremo)."""
        if not sentiment_data:
            return []

        df = pd.DataFrame(sentiment_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        peaks = []
        if "sentiment_score" in df.columns:
            # Define um limiar para picos (pode ser ajustado)
            threshold_positive = df["sentiment_score"].mean() + 2 * df["sentiment_score"].std()
            threshold_negative = df["sentiment_score"].mean() - 2 * df["sentiment_score"].std()

            for index, row in df.iterrows():
                if row["sentiment_score"] > threshold_positive:
                    peaks.append({"timestamp": index.isoformat(), "score": row["sentiment_score"], "type": "positive_peak"})
                elif row["sentiment_score"] < threshold_negative:
                    peaks.append({"timestamp": index.isoformat(), "score": row["sentiment_score"], "type": "negative_peak"})

        return peaks




    def _identify_sentiment_drivers(self, sentiment_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Identifica os principais drivers de sentimento."""
        # Esta √© uma implementa√ß√£o simplificada. Em um cen√°rio real, isso envolveria
        # an√°lise de conte√∫do associado a picos de sentimento, correla√ß√£o com eventos,
        # ou an√°lise de t√≥picos espec√≠ficos que influenciam o sentimento.
        if not sentiment_data:
            return {}

        # Para simula√ß√£o, vamos retornar um driver gen√©rico.
        # Em uma aplica√ß√£o real, voc√™ precisaria de dados mais ricos (e.g., texto original)
        # para associar o sentimento a causas espec√≠ficas.
        return {"main_drivers": ["Conte√∫do textual relevante", "Eventos externos (a serem correlacionados)"]}




    def _gather_topic_temporal_data(self, session_dir: Path) -> List[Dict[str, Any]]:
        """Simula a coleta de dados temporais de t√≥picos."""
        topic_temporal_data = []
        # Em um cen√°rio real, isso leria dados de t√≥picos extra√≠dos ao longo do tempo,
        # possivelmente de diferentes documentos com seus timestamps.
        # Para simula√ß√£o, vamos criar alguns dados fict√≠cios.
        today = datetime.now()
        for i in range(30):
            date = today - timedelta(days=i)
            topic_temporal_data.append({
                "timestamp": date.isoformat(),
                "topic_distribution": {
                    "topic_A": np.random.rand(),
                    "topic_B": np.random.rand(),
                    "topic_C": np.random.rand()
                }
            })
        return topic_temporal_data




    def _analyze_topic_lifecycle(self, topic_temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa o ciclo de vida dos t√≥picos (emerg√™ncia, crescimento, maturidade, decl√≠nio)."""
        if not topic_temporal_data:
            return {}

        # Esta √© uma implementa√ß√£o simplificada. Uma an√°lise real exigiria
        # rastrear a frequ√™ncia e a proemin√™ncia de cada t√≥pico ao longo do tempo.
        
        # Para simula√ß√£o, vamos identificar o t√≥pico mais frequente como 'maduro'
        # e outros como 'emergentes' ou 'em decl√≠nio' com base em sua presen√ßa.
        
        topic_counts = defaultdict(int)
        for data_point in topic_temporal_data:
            for topic, score in data_point["topic_distribution"].items():
                if score > 0.5: # Considera o t√≥pico presente se o score for acima de um limiar
                    topic_counts[topic] += 1
        
        if not topic_counts:
            return {}

        total_data_points = len(topic_temporal_data)
        lifecycle_analysis = {}

        for topic, count in topic_counts.items():
            percentage_presence = (count / total_data_points) * 100
            if percentage_presence > 70: # Exemplo de limiar para t√≥pico maduro
                lifecycle_analysis[topic] = "mature"
            elif percentage_presence > 30:
                lifecycle_analysis[topic] = "growing"
            else:
                lifecycle_analysis[topic] = "emerging_or_declining"

        return lifecycle_analysis




    def _classify_topic_trends(self, topic_temporal_data: List[Dict[str, Any]]) -> Tuple[List[str], List[str], List[str]]:
        """Classifica t√≥picos como emergentes, em decl√≠nio ou est√°veis."""
        if not topic_temporal_data or len(topic_temporal_data) < 2:
            return [], [], []

        # Esta √© uma implementa√ß√£o simplificada. Uma an√°lise real exigiria
        # regress√£o linear ou an√°lise de s√©ries temporais para cada t√≥pico.

        # Para simula√ß√£o, vamos comparar a presen√ßa do t√≥pico no in√≠cio e no fim do per√≠odo.
        first_period_topics = topic_temporal_data[0]["topic_distribution"]
        last_period_topics = topic_temporal_data[-1]["topic_distribution"]

        emerging = []
        declining = []
        stable = []

        for topic in first_period_topics.keys():
            start_score = first_period_topics.get(topic, 0)
            end_score = last_period_topics.get(topic, 0)

            if end_score > start_score * 1.2: # Aumento de 20%
                emerging.append(topic)
            elif end_score < start_score * 0.8: # Queda de 20%
                declining.append(topic)
            else:
                stable.append(topic)
        
        return emerging, declining, stable




    def _analyze_topic_transitions(self, topic_temporal_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa transi√ß√µes entre t√≥picos ao longo do tempo."""
        if not topic_temporal_data or len(topic_temporal_data) < 2:
            return {}

        transitions = defaultdict(lambda: defaultdict(int))

        for i in range(len(topic_temporal_data) - 1):
            current_topics = {t for t, s in topic_temporal_data[i]["topic_distribution"].items() if s > 0.5}
            next_topics = {t for t, s in topic_temporal_data[i+1]["topic_distribution"].items() if s > 0.5}

            for current_topic in current_topics:
                for next_topic in next_topics:
                    if current_topic != next_topic:
                        transitions[current_topic][next_topic] += 1
        
        return {k: dict(v) for k, v in transitions.items()}




    def _gather_engagement_data(self, session_dir: Path) -> List[Dict[str, Any]]:
        """Simula a coleta de dados de engajamento."""
        engagement_data = []
        # Em um cen√°rio real, isso leria dados de intera√ß√µes de usu√°rios, visualiza√ß√µes,
        # cliques, coment√°rios, etc., com seus timestamps.
        # Para simula√ß√£o, vamos criar alguns dados fict√≠cios.
        today = datetime.now()
        for i in range(30):
            date = today - timedelta(days=i)
            engagement_data.append({
                "timestamp": date.isoformat(),
                "views": np.random.randint(100, 10000),
                "likes": np.random.randint(10, 500),
                "comments": np.random.randint(0, 50),
                "shares": np.random.randint(0, 20)
            })
        return engagement_data




    def _calculate_engagement_metrics(self, engagement_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calcula m√©tricas de engajamento."""
        if not engagement_data:
            return {}

        df = pd.DataFrame(engagement_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        metrics = {}
        metrics["total_views"] = df["views"].sum()
        metrics["average_likes_per_view"] = df["likes"].sum() / df["views"].sum() if df["views"].sum() > 0 else 0
        metrics["average_comments_per_view"] = df["comments"].sum() / df["views"].sum() if df["views"].sum() > 0 else 0
        metrics["average_shares_per_view"] = df["shares"].sum() / df["views"].sum() if df["views"].sum() > 0 else 0
        metrics["engagement_rate"] = (df["likes"].sum() + df["comments"].sum() + df["shares"].sum()) / df["views"].sum() if df["views"].sum() > 0 else 0

        return metrics




    def _identify_viral_patterns(self, engagement_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Identifica padr√µes virais (e.g., picos s√∫bitos de engajamento)."""
        if not engagement_data or len(engagement_data) < 5:
            return {}

        df = pd.DataFrame(engagement_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        viral_patterns = {}
        # Exemplo: detec√ß√£o de picos s√∫bitos em visualiza√ß√µes
        if "views" in df.columns:
            # Calcula a m√©dia m√≥vel e o desvio padr√£o das visualiza√ß√µes
            df["views_mean"] = df["views"].rolling(window=3, center=True).mean()
            df["views_std"] = df["views"].rolling(window=3, center=True).std()

            # Identifica pontos onde as visualiza√ß√µes est√£o significativamente acima da m√©dia
            df["is_viral_peak"] = (df["views"] > df["views_mean"] + 2 * df["views_std"])
            
            viral_peaks = df[df["is_viral_peak"]].to_dict(orient="records")
            viral_patterns["viral_peaks_views"] = [{
                "timestamp": p["timestamp"].isoformat(),
                "views": p["views"],
                "likes": p["likes"],
                "comments": p["comments"],
                "shares": p["shares"]
            } for p in viral_peaks]

        return viral_patterns




    def _analyze_audience_behavior(self, engagement_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa o comportamento da audi√™ncia com base nos dados de engajamento."""
        if not engagement_data:
            return {}

        df = pd.DataFrame(engagement_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        audience_behavior = {}

        # Exemplo: dias da semana com maior engajamento
        df["day_of_week"] = df.index.dayofweek
        engagement_by_day = df.groupby("day_of_week")[[ "views", "likes", "comments", "shares"]].sum()
        audience_behavior["engagement_by_day_of_week"] = engagement_by_day.to_dict(orient="index")

        # Exemplo: correla√ß√£o entre diferentes m√©tricas de engajamento
        if len(df.columns) > 1:
            correlation_matrix = df[["views", "likes", "comments", "shares"]].corr().to_dict()
            audience_behavior["engagement_metrics_correlation"] = correlation_matrix

        return audience_behavior




    def _analyze_content_performance(self, engagement_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa a performance do conte√∫do com base nos dados de engajamento."""
        if not engagement_data:
            return {}

        df = pd.DataFrame(engagement_data)
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.set_index("timestamp").sort_index()

        content_performance = {}

        # Exemplo: conte√∫do com maior engajamento (simulado, pois n√£o temos IDs de conte√∫do aqui)
        # Em um cen√°rio real, cada item em engagement_data teria um content_id
        content_performance["top_performing_content_example"] = {
            "most_views": df["views"].max(),
            "most_likes": df["likes"].max(),
            "most_comments": df["comments"].max()
        }

        return content_performance




    def _predict_market_growth(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ o crescimento do mercado com base em insights."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real de crescimento de mercado
        # exigiria dados de mercado externos e modelos econ√¥micos.
        
        # Para simula√ß√£o, vamos usar a tend√™ncia de crescimento temporal e um fator aleat√≥rio.
        growth_rate = insights.get("temporal_trends", {}).get("growth_rates", {}).get("monthly_growth_rate", 0.01)
        
        predicted_growth = {
            "next_quarter_growth_estimate": growth_rate * 3 + np.random.uniform(-0.005, 0.005),
            "next_year_growth_estimate": growth_rate * 12 + np.random.uniform(-0.01, 0.015)
        }
        return predicted_growth




    def _predict_trend_evolution(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ a evolu√ß√£o das tend√™ncias com base em insights."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real de tend√™ncias exigiria
        # modelos de s√©ries temporais mais complexos e dados de tend√™ncias espec√≠ficas.

        # Para simula√ß√£o, vamos usar a acelera√ß√£o da tend√™ncia e a velocidade de mudan√ßa.
        trend_acceleration = insights.get("temporal_trends", {}).get("trend_acceleration", {}).get("average_acceleration", 0)
        velocity_of_change = insights.get("temporal_trends", {}).get("velocity_of_change", {}).get("average_change_per_period", 0)

        predicted_trends = {
            "short_term_trend_direction": "increasing" if velocity_of_change > 0 else "decreasing" if velocity_of_change < 0 else "stable",
            "long_term_trend_acceleration_impact": "accelerating" if trend_acceleration > 0 else "decelerating" if trend_acceleration < 0 else "stable"
        }
        return predicted_trends




    def _predict_sentiment_evolution(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ a evolu√ß√£o do sentimento."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real de sentimento exigiria
        # modelos de s√©ries temporais aplicados aos dados de sentimento.

        # Para simula√ß√£o, vamos usar a tend√™ncia geral do sentimento.
        overall_sentiment_trend = insights.get("sentiment_dynamics", {}).get("overall_sentiment_trend", {})
        linear_trend_slope = overall_sentiment_trend.get("linear_trend_slope", 0)

        predicted_sentiment = {
            "next_period_sentiment_direction": "positive" if linear_trend_slope > 0 else "negative" if linear_trend_slope < 0 else "neutral",
            "sentiment_stability_forecast": "stable" if insights.get("sentiment_dynamics", {}).get("sentiment_volatility", {}).get("rolling_std_dev_sentiment", {}).get(list(insights["sentiment_dynamics"]["sentiment_volatility"]["rolling_std_dev_sentiment"])[-1], 0) < 0.1 else "volatile"
        }
        return predicted_sentiment




    def _predict_engagement_patterns(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ padr√µes de engajamento futuros."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real de engajamento exigiria
        # modelos de s√©ries temporais aplicados a cada m√©trica de engajamento.

        # Para simula√ß√£o, vamos usar as m√©tricas de engajamento atuais e um fator aleat√≥rio.
        current_engagement_metrics = insights.get("engagement_patterns", {}).get("engagement_metrics", {})
        
        predicted_engagement = {
            "predicted_views_next_month": current_engagement_metrics.get("total_views", 0) * (1 + np.random.uniform(-0.05, 0.05)),
            "predicted_engagement_rate_next_month": current_engagement_metrics.get("engagement_rate", 0) * (1 + np.random.uniform(-0.02, 0.02))
        }
        return predicted_engagement




    def _predict_competitive_evolution(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ a evolu√ß√£o do cen√°rio competitivo."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real do cen√°rio competitivo exigiria
        # an√°lise de concorrentes, participa√ß√£o de mercado, lan√ßamentos de produtos, etc.

        # Para simula√ß√£o, vamos inferir a partir da an√°lise de t√≥picos e tend√™ncias.
        emerging_topics = insights.get("topic_evolution", {}).get("emerging_topics", [])
        declining_topics = insights.get("topic_evolution", {}).get("declining_topics", [])

        competitive_evolution = {
            "new_competitor_areas": emerging_topics, # √Åreas onde novos competidores podem surgir
            "weakening_competitor_areas": declining_topics, # √Åreas onde competidores existentes podem enfraquecer
            "overall_competitive_pressure": "stable" # Simula√ß√£o
        }
        if len(emerging_topics) > 0:
            competitive_evolution["overall_competitive_pressure"] = "increasing"
        if len(declining_topics) > 0:
            competitive_evolution["overall_competitive_pressure"] = "decreasing_in_some_areas"

        return competitive_evolution




    def _model_technology_adoption(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela a curva de ado√ß√£o tecnol√≥gica."""
        # Esta √© uma simula√ß√£o. Uma modelagem real exigiria dados hist√≥ricos de ado√ß√£o
        # de tecnologias similares e modelos de difus√£o de inova√ß√£o (e.g., Bass Model).

        # Para simula√ß√£o, vamos retornar uma curva de ado√ß√£o gen√©rica.
        adoption_curve = {
            "innovation_phase": {"start": "T0", "end": "T1", "adoption_rate": "low"},
            "early_adopters_phase": {"start": "T1", "end": "T2", "adoption_rate": "moderate"},
            "early_majority_phase": {"start": "T2", "end": "T3", "adoption_rate": "high"},
            "late_majority_phase": {"start": "T3", "end": "T4", "adoption_rate": "slowing"},
            "laggards_phase": {"start": "T4", "end": "T5", "adoption_rate": "very_low"}
        }
        return adoption_curve




    def _predict_consumer_behavior_shifts(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Prev√™ mudan√ßas no comportamento do consumidor."""
        # Esta √© uma simula√ß√£o. Uma previs√£o real exigiria an√°lise de dados demogr√°ficos,
        # psicogr√°ficos, tend√™ncias de consumo e modelos preditivos complexos.

        # Para simula√ß√£o, vamos usar insights de sentimento e engajamento.
        sentiment_direction = insights.get("predictions", {}).get("sentiment_forecast", {}).get("next_period_sentiment_direction", "neutral")
        engagement_rate_forecast = insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0)

        behavior_shifts = {
            "overall_sentiment_impact": f"Consumer sentiment expected to be {sentiment_direction}",
            "engagement_level_impact": f"Engagement levels expected to be around {engagement_rate_forecast:.2f}",
            "potential_shift_areas": insights.get("textual_insights", {}).get("emerging_themes", []) # Temas emergentes podem indicar novas √°reas de interesse
        }
        return behavior_shifts




    def _create_risk_probability_matrix(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Cria uma matriz de probabilidade de riscos."""
        # Esta √© uma simula√ß√£o. Uma matriz de risco real exigiria identifica√ß√£o
        # de riscos espec√≠ficos e avalia√ß√£o de sua probabilidade e impacto.

        # Para simula√ß√£o, vamos usar a volatilidade do sentimento e a acelera√ß√£o da tend√™ncia.
        sentiment_volatility = insights.get("sentiment_dynamics", {}).get("sentiment_volatility", {}).get("overall_range_sentiment", 0)
        trend_acceleration = insights.get("temporal_trends", {}).get("trend_acceleration", {}).get("average_acceleration", 0)

        risk_matrix = {
            "risk_1_name": "Volatilidade de Sentimento",
            "risk_1_probability": "high" if sentiment_volatility > 0.5 else "medium" if sentiment_volatility > 0.2 else "low",
            "risk_1_impact": "high" if sentiment_volatility > 0.5 else "medium" if sentiment_volatility > 0.2 else "low",
            "risk_2_name": "Mudan√ßa Acelerada de Tend√™ncia",
            "risk_2_probability": "high" if abs(trend_acceleration) > 0.1 else "medium" if abs(trend_acceleration) > 0.05 else "low",
            "risk_2_impact": "high" if abs(trend_acceleration) > 0.1 else "medium" if abs(trend_acceleration) > 0.05 else "low"
        }
        return risk_matrix




    def _create_opportunity_timeline(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Cria uma linha do tempo de oportunidades."""
        # Esta √© uma simula√ß√£o. Uma linha do tempo de oportunidades real exigiria
        # identifica√ß√£o de oportunidades espec√≠ficas e sua janela de tempo.

        # Para simula√ß√£o, vamos usar os temas emergentes e as previs√µes de crescimento.
        emerging_themes = insights.get("textual_insights", {}).get("emerging_themes", [])
        market_growth_forecast = insights.get("predictions", {}).get("market_growth_forecast", {})

        opportunity_timeline = {
            "opportunity_1": {
                "name": f"Explorar tema emergente: {emerging_themes[0] if emerging_themes else 'N/A'}",
                "timing": "short-term",
                "potential_impact": "high"
            },
            "opportunity_2": {
                "name": f"Capitalizar crescimento de mercado: {market_growth_forecast.get('next_quarter_growth_estimate', 0):.2f}",
                "timing": "mid-term",
                "potential_impact": "medium"
            }
        }
        return opportunity_timeline




    def _identify_strategic_inflection_points(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Identifica pontos de inflex√£o estrat√©gica."""
        # Esta √© uma simula√ß√£o. A identifica√ß√£o de pontos de inflex√£o estrat√©gica
        # √© complexa e geralmente envolve an√°lise de m√∫ltiplos fatores e julgamento humano.

        # Para simula√ß√£o, vamos usar a detec√ß√£o de anomalias e a acelera√ß√£o da tend√™ncia.
        anomalies = insights.get("temporal_trends", {}).get("anomaly_detection", [])
        trend_acceleration = insights.get("temporal_trends", {}).get("trend_acceleration", {}).get("average_acceleration", 0)

        inflection_points = {
            "anomalies_as_potential_inflection_points": anomalies,
            "trend_acceleration_indicator": "significant_change" if abs(trend_acceleration) > 0.1 else "stable",
            "strategic_implications": "Reavaliar estrat√©gia se anomalias ou acelera√ß√£o de tend√™ncia forem significativas."
        }
        return inflection_points




    def _model_base_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela o cen√°rio base (o mais prov√°vel)."""
        # Este √© um cen√°rio simulado. Um cen√°rio base real seria constru√≠do
        # com base nas proje√ß√µes mais realistas das tend√™ncias atuais.

        return {
            "description": "Cen√°rio mais prov√°vel, com base nas tend√™ncias atuais e previs√µes de crescimento.",
            "key_metrics_projection": {
                "market_growth": insights.get("predictions", {}).get("market_growth_forecast", {}).get("next_year_growth_estimate", 0),
                "overall_sentiment": insights.get("predictions", {}).get("sentiment_forecast", {}).get("next_period_sentiment_direction", "neutral"),
                "engagement_rate": insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0)
            }
        }




    def _model_optimistic_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela o cen√°rio otimista."""
        # Simula√ß√£o de um cen√°rio otimista, com melhorias em m√©tricas chave.
        base_scenario = self._model_base_scenario(insights)
        optimistic_growth = base_scenario["key_metrics_projection"]["market_growth"] * 1.2
        optimistic_engagement = base_scenario["key_metrics_projection"]["engagement_rate"] * 1.1

        return {
            "description": "Cen√°rio otimista, com condi√ß√µes de mercado favor√°veis e alto engajamento.",
            "key_metrics_projection": {
                "market_growth": optimistic_growth,
                "overall_sentiment": "highly_positive",
                "engagement_rate": optimistic_engagement
            }
        }




    def _model_pessimistic_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela o cen√°rio pessimista."""
        # Simula√ß√£o de um cen√°rio pessimista, com decl√≠nio em m√©tricas chave.
        base_scenario = self._model_base_scenario(insights)
        pessimistic_growth = base_scenario["key_metrics_projection"]["market_growth"] * 0.5
        pessimistic_engagement = base_scenario["key_metrics_projection"]["engagement_rate"] * 0.7

        return {
            "description": "Cen√°rio pessimista, com desacelera√ß√£o do mercado e baixo engajamento.",
            "key_metrics_projection": {
                "market_growth": pessimistic_growth,
                "overall_sentiment": "highly_negative",
                "engagement_rate": pessimistic_engagement
            }
        }




    def _model_disruptive_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela um cen√°rio disruptivo."""
        # Simula√ß√£o de um cen√°rio disruptivo, com mudan√ßas radicais.
        return {
            "description": "Cen√°rio disruptivo, com uma nova tecnologia ou concorrente mudando drasticamente o mercado.",
            "key_metrics_projection": {
                "market_growth": 0.1, # Crescimento baixo devido √† incerteza
                "overall_sentiment": "highly_volatile",
                "engagement_rate": 0.05 # Engajamento baixo devido √† fragmenta√ß√£o
            },
            "disruptive_elements": insights.get("topic_evolution", {}).get("emerging_topics", [])
        }




    def _model_regulatory_change_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela um cen√°rio de mudan√ßa regulat√≥ria."""
        return {
            "description": "Cen√°rio de mudan√ßa regulat√≥ria, impactando opera√ß√µes e conformidade.",
            "key_metrics_projection": {
                "market_growth": insights.get("predictions", {}).get("market_growth_forecast", {}).get("next_year_growth_estimate", 0) * 0.8, # Impacto negativo
                "overall_sentiment": "neutral_to_negative",
                "engagement_rate": insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0) * 0.9
            },
            "regulatory_impact": "Aumento de custos de conformidade, novas restri√ß√µes."
        }




    def _model_economic_crisis_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela um cen√°rio de crise econ√¥mica."""
        return {
            "description": "Cen√°rio de crise econ√¥mica, com retra√ß√£o do mercado e poder de compra reduzido.",
            "key_metrics_projection": {
                "market_growth": insights.get("predictions", {}).get("market_growth_forecast", {}).get("next_year_growth_estimate", 0) * 0.3, # Forte retra√ß√£o
                "overall_sentiment": "highly_negative",
                "engagement_rate": insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0) * 0.7
            },
            "economic_impact": "Redu√ß√£o de gastos do consumidor, aumento do desemprego."
        }




    def _model_technology_breakthrough_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela um cen√°rio de avan√ßo tecnol√≥gico."""
        return {
            "description": "Cen√°rio de avan√ßo tecnol√≥gico, com uma nova tecnologia transformando o setor.",
            "key_metrics_projection": {
                "market_growth": insights.get("predictions", {}).get("market_growth_forecast", {}).get("next_year_growth_estimate", 0) * 1.5, # Acelera√ß√£o do crescimento
                "overall_sentiment": "highly_positive",
                "engagement_rate": insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0) * 1.3
            },
            "tech_impact": "Novas oportunidades de produto, obsolesc√™ncia de tecnologias existentes."
        }




    def _model_competitive_disruption_scenario(self, insights: Dict[str, Any]) -> Dict[str, Any]:
        """Modela um cen√°rio de disrup√ß√£o competitiva."""
        return {
            "description": "Cen√°rio de disrup√ß√£o competitiva, com um novo player mudando as regras do jogo.",
            "key_metrics_projection": {
                "market_growth": insights.get("predictions", {}).get("market_growth_forecast", {}).get("next_year_growth_estimate", 0) * 0.7, # Perda de market share
                "overall_sentiment": "negative",
                "engagement_rate": insights.get("predictions", {}).get("engagement_predictions", {}).get("predicted_engagement_rate_next_month", 0) * 0.8
            },
            "competitive_impact": "Perda de clientes, necessidade de inova√ß√£o r√°pida."
        }




    def _calculate_scenario_probabilities(self, insights: Dict[str, Any]) -> Dict[str, float]:
        """Calcula as probabilidades dos cen√°rios."""
        # Esta √© uma simula√ß√£o. As probabilidades de cen√°rio seriam baseadas
        # em an√°lises de risco, dados hist√≥ricos e julgamento de especialistas.

        # Para simula√ß√£o, vamos atribuir probabilidades arbitr√°rias que somam 1.
        return {
            "base_scenario": 0.5,
            "optimistic_scenario": 0.2,
            "pessimistic_scenario": 0.15,
            "disruptive_scenario": 0.05,
            "regulatory_change_scenario": 0.03,
            "economic_crisis_scenario": 0.03,
            "technology_breakthrough_scenario": 0.02,
            "competitive_disruption_scenario": 0.02
        }




    def _create_scenario_impact_matrix(self, scenarios: Dict[str, Any]) -> Dict[str, Any]:
        """Cria uma matriz de impacto dos cen√°rios."""
        # Esta √© uma simula√ß√£o. Uma matriz de impacto real avaliaria o impacto
        # de cada cen√°rio em diferentes √°reas de neg√≥cio (financeiro, operacional, reputa√ß√£o, etc.).

        impact_matrix = {}
        for scenario_name, scenario_data in scenarios.items():
            if "key_metrics_projection" in scenario_data:
                impact_matrix[scenario_name] = {
                    "market_growth_impact": scenario_data["key_metrics_projection"].get("market_growth", "N/A"),
                    "sentiment_impact": scenario_data["key_metrics_projection"].get("overall_sentiment", "N/A"),
                    "engagement_impact": scenario_data["key_metrics_projection"].get("engagement_rate", "N/A")
                }
        return impact_matrix




    def _generate_contingency_plans(self, scenarios: Dict[str, Any]) -> Dict[str, Any]:
        """Gera planos de conting√™ncia para cen√°rios espec√≠ficos."""
        # Esta √© uma simula√ß√£o. Planos de conting√™ncia reais seriam detalhados
        # e espec√≠ficos para cada risco e cen√°rio identificado.

        contingency_plans = {}
        if "pessimistic_scenario" in scenarios:
            contingency_plans["pessimistic_scenario_plan"] = {
                "action_1": "Reduzir custos operacionais em X%",
                "action_2": "Focar em reten√ß√£o de clientes",
                "action_3": "Reavaliar investimentos de alto risco"
            }
        if "disruptive_scenario" in scenarios:
            contingency_plans["disruptive_scenario_plan"] = {
                "action_1": "Investir em P&D para novas tecnologias",
                "action_2": "Formar parcerias estrat√©gicas com inovadores",
                "action_3": "Diversificar portf√≥lio de produtos/servi√ßos"
            }
        if "economic_crisis_scenario" in scenarios:
            contingency_plans["economic_crisis_scenario_plan"] = {
                "action_1": "Otimizar fluxo de caixa",
                "action_2": "Negociar prazos com fornecedores e clientes",
                "action_3": "Explorar mercados de menor custo"
            }
        return contingency_plans


