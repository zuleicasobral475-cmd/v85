#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Unified Analysis Engine
Motor de an√°lise unificado que combina todas as capacidades
"""

import logging
import time
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
from services.ai_manager import ai_manager
from services.unified_search_manager import unified_search_manager
from services.robust_content_extractor import robust_content_extractor
from services.pymupdf_client import pymupdf_client
from services.exa_client import exa_client
from services.mental_drivers_architect import mental_drivers_architect
from services.visual_proofs_generator import visual_proofs_generator
from services.anti_objection_system import anti_objection_system
from services.pre_pitch_architect import pre_pitch_architect
from services.archaeological_master import archaeological_master
from services.visceral_master_agent import visceral_master
from services.visual_proofs_director import visual_proofs_director
from services.forensic_cpl_analyzer import forensic_cpl_analyzer
from services.visceral_leads_engineer import visceral_leads_engineer
from services.pre_pitch_architect_advanced import pre_pitch_architect_advanced
from services.auto_save_manager import auto_save_manager, salvar_etapa, salvar_erro

logger = logging.getLogger(__name__)

class UnifiedAnalysisEngine:
    """Motor de an√°lise unificado com todas as capacidades"""

    def __init__(self):
        """Inicializa o motor unificado"""
        self.analysis_types = {
            'standard': 'An√°lise Padr√£o Ultra-Detalhada',
            'archaeological': 'An√°lise Arqueol√≥gica (12 Camadas)',
            'forensic_cpl': 'An√°lise Forense de CPL',
            'visceral_leads': 'Engenharia Reversa de Leads',
            'pre_pitch': 'Orquestra√ß√£o de Pr√©-Pitch',
            'complete': 'An√°lise Completa Unificada'
        }

        self.available_agents = {
            'arqueologist': archaeological_master,
            'visceral_master': visceral_master,
            'visual_director': visual_proofs_director,
            'drivers_architect': mental_drivers_architect,
            'anti_objection': anti_objection_system,
            'pre_pitch_architect': pre_pitch_architect,
            'forensic_cpl': forensic_cpl_analyzer,
            'visceral_leads': visceral_leads_engineer,
            'pre_pitch_advanced': pre_pitch_architect_advanced
        }

        logger.info("üöÄ Unified Analysis Engine inicializado")

    def _validate_required_apis(self):
        """Valida se as APIs obrigat√≥rias est√£o dispon√≠veis"""

        # Verifica se h√° pelo menos uma IA dispon√≠vel
        ai_available = False
        for provider_name, provider in ai_manager.providers.items():
            if provider['available']:
                ai_available = True
                break

        if not ai_available:
            raise Exception(
                "‚ùå NENHUMA API DE IA CONFIGURADA\n\n"
                "Configure pelo menos uma:\n"
                "- GEMINI_API_KEY (Recomendado)\n"
                "- OPENAI_API_KEY\n"
                "- GROQ_API_KEY\n"
                "- HUGGINGFACE_API_KEY"
            )

        # Verifica se h√° pesquisa web dispon√≠vel
        search_status = unified_search_manager.get_provider_status()
        search_available = any(status.get('available', False) for status in search_status.values())

        if not search_available:
            raise Exception(
                "‚ùå NENHUMA API DE PESQUISA CONFIGURADA\n\n"
                "Configure pelo menos uma:\n"
                "- EXA_API_KEY (Recomendado)\n"
                "- GOOGLE_API_KEY + GOOGLE_CSE_ID\n"
                "- SERPER_API_KEY"
            )

        logger.info("‚úÖ APIs obrigat√≥rias validadas com sucesso")

    def execute_unified_analysis(
        self,
        data: Dict[str, Any],
        analysis_type: str = 'complete',
        session_id: str = None,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise unificada com tipo especificado"""

        # VALIDA√á√ÉO CR√çTICA: APIs obrigat√≥rias
        self._validate_required_apis()

        logger.info(f"üöÄ Iniciando an√°lise unificada: {analysis_type}")
        start_time = time.time()

        # Inicia sess√£o se n√£o fornecida
        if not session_id:
            session_id = auto_save_manager.iniciar_sessao()

        # Salva in√≠cio da an√°lise
        salvar_etapa("analise_unificada_iniciada", {
            "data": data,
            "analysis_type": analysis_type,
            "session_id": session_id,
            "available_agents": list(self.available_agents.keys())
        }, categoria="analise_completa")

        try:
            if analysis_type == 'complete':
                return self._execute_complete_unified_analysis(data, session_id, progress_callback)
            elif analysis_type == 'archaeological':
                return self._execute_archaeological_analysis(data, session_id, progress_callback)
            elif analysis_type == 'forensic_cpl':
                return self._execute_forensic_cpl_analysis(data, session_id, progress_callback)
            elif analysis_type == 'visceral_leads':
                return self._execute_visceral_leads_analysis(data, session_id, progress_callback)
            elif analysis_type == 'pre_pitch':
                return self._execute_pre_pitch_analysis(data, session_id, progress_callback)
            else:
                return self._execute_standard_analysis(data, session_id, progress_callback)

        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise unificada: {e}")
            salvar_erro("analise_unificada_erro", e, contexto=data)
            raise e

    def _execute_complete_unified_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise completa unificada com todos os agentes"""

        if progress_callback:
            progress_callback(1, "üîç Iniciando an√°lise completa unificada...")

        # 1. Pesquisa unificada
        if progress_callback:
            progress_callback(2, "üåê Executando pesquisa unificada (Exa + Google + Serper)...")

        search_query = data.get('query') or f"mercado {data.get('segmento', 'neg√≥cios')} Brasil 2024"
        search_results = unified_search_manager.unified_search(search_query, max_results=25, context=data, session_id=session_id)

        # 2. Extra√ß√£o de conte√∫do avan√ßada
        if progress_callback:
            progress_callback(3, "üìÑ Extraindo conte√∫do com PyMuPDF Pro + extratores robustos...")

        extracted_content = self._extract_unified_content(search_results, session_id)

        # 3. An√°lise arqueol√≥gica
        if progress_callback:
            progress_callback(4, "üî¨ Executando an√°lise arqueol√≥gica (12 camadas)...")

        archaeological_analysis = archaeological_master.execute_archaeological_analysis(
            data, 
            research_context=json.dumps(extracted_content, ensure_ascii=False)[:15000],
            session_id=session_id
        )

        # 4. Engenharia reversa visceral
        if progress_callback:
            progress_callback(5, "üß† Executando engenharia reversa visceral...")

        visceral_analysis = visceral_master.execute_visceral_analysis(
            data,
            research_data=extracted_content,
            session_id=session_id
        )

        # 5. Arsenal de drivers mentais
        if progress_callback:
            progress_callback(6, "‚öôÔ∏è Criando arsenal de drivers mentais...")

        avatar_data = visceral_analysis.get('avatar_visceral_ultra', {})
        if not avatar_data:
            avatar_data = archaeological_analysis.get('avatar_arqueologico_ultra', {})

        drivers_system = mental_drivers_architect.generate_complete_drivers_system(avatar_data, data)

        # 6. Arsenal de PROVIs
        if progress_callback:
            progress_callback(7, "üé≠ Criando arsenal de PROVIs devastadoras...")

        concepts_to_prove = self._extract_concepts_for_proofs(avatar_data, drivers_system, data)
        provis_system = visual_proofs_director.execute_provis_creation(
            concepts_to_prove,
            avatar_data,
            drivers_system,
            data,
            session_id
        )

        # 7. Sistema anti-obje√ß√£o
        if progress_callback:
            progress_callback(8, "üõ°Ô∏è Construindo sistema anti-obje√ß√£o...")

        objections_list = avatar_data.get('muralhas_desconfianca_objecoes', [])
        if not objections_list:
            objections_list = [
                "N√£o tenho tempo para implementar isso agora",
                "Preciso pensar melhor sobre o investimento",
                "Meu caso √© muito espec√≠fico",
                "J√° tentei outras coisas e n√£o deram certo"
            ]

        anti_objection_system_result = anti_objection_system.generate_complete_anti_objection_system(
            objections_list, avatar_data, data
        )

        # 8. Pr√©-pitch invis√≠vel
        if progress_callback:
            progress_callback(9, "üéØ Orquestrando pr√©-pitch invis√≠vel...")

        drivers_list = drivers_system.get('drivers_customizados', [])
        pre_pitch_system = pre_pitch_architect_advanced.orchestrate_psychological_symphony(
            drivers_list, avatar_data, 
            data.get('event_structure', 'Webinar/Live/Evento'),
            data.get('product_offer', f"Produto: {data.get('produto', 'N/A')} - Pre√ßo: R$ {data.get('preco', 'N/A')}"),
            session_id
        )

        # 9. Consolida√ß√£o final
        if progress_callback:
            progress_callback(10, "‚ú® Consolidando an√°lise unificada...")

        unified_analysis = {
            'tipo_analise': 'completa_unificada',
            'projeto_dados': data,
            'pesquisa_unificada': search_results,
            'conteudo_extraido': extracted_content,
            'analise_arqueologica': archaeological_analysis,
            'engenharia_reversa_visceral': visceral_analysis,
            'arsenal_drivers_mentais': drivers_system,
            'arsenal_provas_visuais': provis_system,
            'sistema_anti_objecao': anti_objection_system_result,
            'pre_pitch_invisivel': pre_pitch_system,
            'agentes_utilizados': [
                'ARQUE√ìLOGO MESTRE DA PERSUAS√ÉO',
                'MESTRE DA PERSUAS√ÉO VISCERAL',
                'ARQUITETO DE DRIVERS MENTAIS',
                'DIRETOR SUPREMO DE EXPERI√äNCIAS',
                'ESPECIALISTA EM PSICOLOGIA DE VENDAS',
                'MESTRE DO PR√â-PITCH INVIS√çVEL'
            ],
            'tecnologias_utilizadas': [
                'Exa Neural Search',
                'Google Custom Search',
                'PyMuPDF Pro',
                'Gemini 2.5 Pro',
                'Sistema de Extra√ß√£o Robusto'
            ]
        }

        # Adiciona metadados finais
        processing_time = time.time() - start_time
        unified_analysis['metadata_unificado'] = {
            'processing_time_seconds': processing_time,
            'processing_time_formatted': f"{int(processing_time // 60)}m {int(processing_time % 60)}s",
            'analysis_engine': 'ARQV30 Enhanced v2.0 - UNIFIED',
            'generated_at': datetime.now().isoformat(),
            'session_id': session_id,
            'providers_used': len(search_results.get('provider_results', {})),
            'total_sources': search_results.get('statistics', {}).get('total_results', 0),
            'brazilian_sources': search_results.get('statistics', {}).get('brazilian_sources', 0),
            'exa_enhanced': exa_client.is_available(),
            'pymupdf_pro': pymupdf_client.is_available(),
            'analysis_completeness': 'MAXIMUM'
        }

        # Salva an√°lise unificada final
        salvar_etapa("analise_unificada_final", unified_analysis, categoria="analise_completa")

        logger.info(f"‚úÖ An√°lise unificada conclu√≠da em {processing_time:.2f}s")
        return unified_analysis

    def _extract_unified_content(self, search_results: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Extrai conte√∫do usando todos os extratores dispon√≠veis"""

        results = search_results.get('results', [])
        extracted_content = []
        pdf_content = []

        for i, result in enumerate(results[:15]):  # Top 15 resultados
            url = result.get('url', '')

            try:
                # Verifica se √© PDF
                if url.lower().endswith('.pdf') or 'pdf' in url.lower():
                    # Usa PyMuPDF Pro para PDFs
                    if pymupdf_client.is_available():
                        pdf_result = pymupdf_client.extract_from_url(url)
                        if pdf_result['success']:
                            pdf_content.append({
                                'url': url,
                                'title': result.get('title', ''),
                                'content': pdf_result['text'],
                                'metadata': pdf_result['metadata'],
                                'statistics': pdf_result['statistics'],
                                'extraction_method': 'PyMuPDF_Pro'
                            })
                            continue

                # Usa extrator robusto para p√°ginas web
                content = robust_content_extractor.extract_content(url)
                if content and len(content) > 200:
                    extracted_content.append({
                        'url': url,
                        'title': result.get('title', ''),
                        'content': content,
                        'source': result.get('source', 'unknown'),
                        'is_brazilian': result.get('is_brazilian', False),
                        'is_preferred': result.get('is_preferred', False),
                        'extraction_method': 'robust_extractor'
                    })

                # Delay para rate limiting
                time.sleep(0.3)

            except Exception as e:
                logger.error(f"‚ùå Erro ao extrair {url}: {e}")
                continue

        # Combina conte√∫do extra√≠do
        combined_content = {
            'web_content': extracted_content,
            'pdf_content': pdf_content,
            'statistics': {
                'total_web_pages': len(extracted_content),
                'total_pdf_pages': len(pdf_content),
                'total_content_length': sum(len(item['content']) for item in extracted_content + pdf_content),
                'extraction_success_rate': (len(extracted_content) + len(pdf_content)) / len(results) * 100 if results else 0
            }
        }

        # Salva conte√∫do extra√≠do
        salvar_etapa("conteudo_unificado_extraido", combined_content, categoria="pesquisa_web")

        return combined_content

    def _extract_concepts_for_proofs(
        self, 
        avatar_data: Dict[str, Any], 
        drivers_data: Dict[str, Any], 
        context_data: Dict[str, Any]
    ) -> List[str]:
        """Extrai conceitos que precisam de prova visual"""

        concepts = []

        # Conceitos do avatar
        if avatar_data.get('feridas_abertas_inconfessaveis'):
            concepts.extend(avatar_data['feridas_abertas_inconfessaveis'][:5])

        if avatar_data.get('sonhos_proibidos_ardentes'):
            concepts.extend(avatar_data['sonhos_proibidos_ardentes'][:5])

        # Conceitos dos drivers
        if drivers_data.get('drivers_customizados'):
            for driver in drivers_data['drivers_customizados'][:3]:
                concepts.append(driver.get('nome', 'Driver Mental'))

        # Conceitos gerais cr√≠ticos
        concepts.extend([
            "Efic√°cia do m√©todo",
            "Transforma√ß√£o real poss√≠vel",
            "ROI do investimento",
            "Diferencial da concorr√™ncia",
            "Tempo para resultados"
        ])

        return concepts[:12]  # M√°ximo 12 conceitos

    def _execute_archaeological_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa apenas an√°lise arqueol√≥gica"""

        if progress_callback:
            progress_callback(1, "üî¨ Iniciando an√°lise arqueol√≥gica...")

        # Pesquisa unificada
        search_query = data.get('query') or f"mercado {data.get('segmento', 'neg√≥cios')} Brasil 2024"
        search_results = unified_search_manager.unified_search(search_query, context=data, session_id=session_id)

        # An√°lise arqueol√≥gica
        archaeological_result = archaeological_master.execute_archaeological_analysis(
            data,
            research_context=json.dumps(search_results, ensure_ascii=False)[:15000],
            session_id=session_id
        )

        return {
            'tipo_analise': 'arqueologica',
            'projeto_dados': data,
            'pesquisa_unificada': search_results,
            'analise_arqueologica': archaeological_result,
            'metadata': {
                'analysis_type': 'archaeological',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_forensic_cpl_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise forense de CPL"""

        if progress_callback:
            progress_callback(1, "üî¨ Iniciando an√°lise forense de CPL...")

        transcription = data.get('transcription', '')
        context_data = {
            'contexto_estrategico': data.get('contexto_estrategico', ''),
            'objetivo_cpl': data.get('objetivo_cpl', ''),
            'formato': data.get('formato', ''),
            'temperatura_audiencia': data.get('temperatura_audiencia', '')
        }

        forensic_result = forensic_cpl_analyzer.analyze_cpl_forensically(
            transcription, context_data, session_id
        )

        return {
            'tipo_analise': 'forense_cpl',
            'projeto_dados': data,
            'analise_forense_cpl': forensic_result,
            'metadata': {
                'analysis_type': 'forensic_cpl',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_visceral_leads_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa engenharia reversa de leads"""

        if progress_callback:
            progress_callback(1, "üß† Iniciando engenharia reversa visceral...")

        leads_data = data.get('leads_data', '')
        context_data = {
            'produto_servico': data.get('produto_servico', ''),
            'principais_perguntas': data.get('principais_perguntas', ''),
            'numero_respostas': data.get('numero_respostas', 0)
        }

        visceral_result = visceral_leads_engineer.reverse_engineer_leads(
            leads_data, context_data, session_id
        )

        return {
            'tipo_analise': 'visceral_leads',
            'projeto_dados': data,
            'engenharia_reversa_leads': visceral_result,
            'metadata': {
                'analysis_type': 'visceral_leads',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_pre_pitch_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa orquestra√ß√£o de pr√©-pitch"""

        if progress_callback:
            progress_callback(1, "üéØ Iniciando orquestra√ß√£o de pr√©-pitch...")

        selected_drivers = data.get('selected_drivers', [])
        avatar_data = data.get('avatar_data', {})
        event_structure = data.get('event_structure', '')
        product_offer = data.get('product_offer', '')

        pre_pitch_result = pre_pitch_architect_advanced.orchestrate_psychological_symphony(
            selected_drivers, avatar_data, event_structure, product_offer, session_id
        )

        return {
            'tipo_analise': 'pre_pitch',
            'projeto_dados': data,
            'orquestracao_pre_pitch': pre_pitch_result,
            'metadata': {
                'analysis_type': 'pre_pitch',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _execute_standard_analysis(
        self,
        data: Dict[str, Any],
        session_id: str,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """Executa an√°lise padr√£o unificada"""

        if progress_callback:
            progress_callback(1, "üîç Iniciando an√°lise padr√£o unificada...")

        # Pesquisa unificada
        search_query = data.get('query') or f"mercado {data.get('segmento', 'neg√≥cios')} Brasil 2024"
        search_results = unified_search_manager.unified_search(search_query, context=data, session_id=session_id)

        # Extra√ß√£o de conte√∫do
        extracted_content = self._extract_unified_content(search_results, session_id)

        # An√°lise com IA
        analysis_prompt = self._build_unified_analysis_prompt(data, extracted_content)
        ai_response = ai_manager.generate_analysis(analysis_prompt, max_tokens=8192)

        if not ai_response:
            raise Exception("IA n√£o respondeu para an√°lise unificada")

        # Processa resposta
        ai_analysis = self._process_ai_response(ai_response, data)

        return {
            'tipo_analise': 'padrao_unificada',
            'projeto_dados': data,
            'pesquisa_unificada': search_results,
            'conteudo_extraido': extracted_content,
            'analise_ia': ai_analysis,
            'metadata': {
                'analysis_type': 'standard_unified',
                'session_id': session_id,
                'generated_at': datetime.now().isoformat()
            }
        }

    def _build_unified_analysis_prompt(self, data: Dict[str, Any], content: Dict[str, Any]) -> str:
        """Constr√≥i prompt unificado para an√°lise"""

        web_content = content.get('web_content', [])
        pdf_content = content.get('pdf_content', [])

        content_summary = ""

        # Resumo do conte√∫do web
        if web_content:
            content_summary += "CONTE√öDO WEB EXTRA√çDO:\n"
            for i, item in enumerate(web_content[:10], 1):
                content_summary += f"FONTE {i}: {item['title']}\n"
                content_summary += f"URL: {item['url']}\n"
                content_summary += f"Conte√∫do: {item['content'][:1500]}\n\n"

        # Resumo do conte√∫do PDF
        if pdf_content:
            content_summary += "CONTE√öDO PDF EXTRA√çDO:\n"
            for i, item in enumerate(pdf_content[:5], 1):
                content_summary += f"PDF {i}: {item['title']}\n"
                content_summary += f"P√°ginas: {item['statistics']['pages']}\n"
                content_summary += f"Conte√∫do: {item['content'][:2000]}\n\n"

        prompt = f"""
# AN√ÅLISE UNIFICADA ULTRA-DETALHADA - ARQV30 ENHANCED v2.0

Voc√™ √© o DIRETOR SUPREMO DE AN√ÅLISE UNIFICADA, especialista de elite que combina todas as metodologias.

## DADOS DO PROJETO:
- **Segmento**: {data.get('segmento', 'N√£o informado')}
- **Produto/Servi√ßo**: {data.get('produto', 'N√£o informado')}
- **P√∫blico-Alvo**: {data.get('publico', 'N√£o informado')}
- **Pre√ßo**: R$ {data.get('preco', 'N√£o informado')}
- **Objetivo de Receita**: R$ {data.get('objetivo_receita', 'N√£o informado')}

## PESQUISA UNIFICADA REALIZADA:
{content_summary[:15000]}

## GERE AN√ÅLISE UNIFICADA COMPLETA:

```json
{{
  "avatar_unificado": {{
    "nome_ficticio": "Nome espec√≠fico baseado em dados reais",
    "perfil_demografico_completo": {{
      "idade": "Faixa et√°ria com dados reais",
      "genero": "Distribui√ß√£o real",
      "renda": "Faixa de renda real",
      "escolaridade": "N√≠vel educacional real",
      "localizacao": "Regi√µes geogr√°ficas reais"
    }},
    "perfil_psicografico_profundo": {{
      "personalidade": "Tra√ßos dominantes reais",
      "valores": "Valores e cren√ßas reais",
      "comportamento_compra": "Processo real de decis√£o",
      "medos_profundos": "Medos reais documentados",
      "aspiracoes_secretas": "Aspira√ß√µes reais"
    }},
    "dores_viscerais_unificadas": [
      "Lista de 15-20 dores espec√≠ficas baseadas em dados reais"
    ],
    "desejos_secretos_unificados": [
      "Lista de 15-20 desejos profundos baseados em estudos"
    ],
    "jornada_emocional_completa": {{
      "consciencia": "Como toma consci√™ncia baseado em dados",
      "consideracao": "Processo real de avalia√ß√£o",
      "decisao": "Fatores decisivos reais",
      "pos_compra": "Experi√™ncia p√≥s-compra real"
    }}
  }},

  "posicionamento_unificado": {{
    "proposta_valor_unica": "Proposta irresist√≠vel baseada em gaps",
    "diferenciais_competitivos": [
      "Lista de diferenciais √∫nicos e defens√°veis"
    ],
    "mensagem_central": "Mensagem principal que resume tudo",
    "estrategia_oceano_azul": "Como criar mercado sem concorr√™ncia"
  }},

  "insights_unificados": [
    "Lista de 25-30 insights √∫nicos e ultra-valiosos baseados na an√°lise completa"
  ],

  "estrategia_implementacao": {{
    "fase_1_preparacao": {{
      "duracao": "Tempo necess√°rio",
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }},
    "fase_2_execucao": {{
      "duracao": "Tempo necess√°rio", 
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }},
    "fase_3_otimizacao": {{
      "duracao": "Tempo necess√°rio",
      "atividades": ["Lista de atividades espec√≠ficas"],
      "investimento": "Investimento necess√°rio"
    }}
  }}
}}
```

CR√çTICO: Use APENAS dados REAIS da pesquisa unificada. Combine insights de todas as fontes.
"""

        return prompt

    def _process_ai_response(self, response: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Processa resposta da IA"""

        try:
            # Extrai JSON da resposta
            clean_text = response.strip()

            if "```json" in clean_text:
                start = clean_text.find("```json") + 7
                end = clean_text.rfind("```")
                clean_text = clean_text[start:end].strip()

            # Parseia JSON
            analysis = json.loads(clean_text)

            # Adiciona metadados
            analysis['metadata_ai'] = {
                'generated_at': datetime.now().isoformat(),
                'provider_used': 'unified_ai_manager',
                'analysis_type': 'unified_complete'
            }

            return analysis

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erro ao parsear JSON: {e}")
            return self._create_fallback_analysis(data)

    def _create_fallback_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """FALLBACK REMOVIDO - Sistema exige an√°lise com dados reais"""

        raise Exception(
            "‚ùå AN√ÅLISE COM DADOS REAIS OBRIGAT√ìRIA\n\n"
            "O sistema n√£o aceita mais fallbacks ou simula√ß√µes.\n"
            "Configure pelo menos uma API de IA (Gemini, OpenAI, Groq) e uma de pesquisa (Exa, Google).\n\n"
            "APIs necess√°rias:\n"
            "- GEMINI_API_KEY ou OPENAI_API_KEY ou GROQ_API_KEY\n"
            "- EXA_API_KEY ou GOOGLE_CSE_ID + GOOGLE_API_KEY\n\n"
            "Todas as an√°lises devem ser baseadas em dados reais coletados da web."
        )

    def get_analysis_capabilities(self) -> Dict[str, Any]:
        """Retorna capacidades de an√°lise dispon√≠veis"""

        return {
            'analysis_types': self.analysis_types,
            'available_agents': {
                name: {
                    'available': True,
                    'description': f'Agente {name} dispon√≠vel'
                } for name in self.available_agents.keys()
            },
            'search_providers': unified_search_manager.get_provider_status(),
            'extraction_capabilities': {
                'web_extraction': robust_content_extractor is not None,
                'pdf_extraction': pymupdf_client.is_available(),
                'exa_neural_search': exa_client.is_available()
            },
            'ai_providers': ai_manager.get_provider_status() if ai_manager else {}
        }

    def _extrair_secao(self, resultados: Dict[str, Any], chave: str, default: Any) -> Any:
        """Extrai uma se√ß√£o espec√≠fica dos resultados"""
        try:
            return resultados.get(chave, default)
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair se√ß√£o {chave}: {e}")
            return default

    def _extrair_metricas(self, resultados: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai m√©tricas dos resultados"""
        metricas = {
            'total_fontes': 0,
            'qualidade_dados': 'm√©dia',
            'cobertura_analise': 0.0,
            'tempo_analise': 0,
            'componentes_executados': []
        }

        try:
            if 'web_search' in resultados:
                web_data = resultados['web_search']
                if isinstance(web_data, dict):
                    metricas['total_fontes'] = len(web_data.get('resultados', []))

            # Calcula cobertura da an√°lise
            componentes_esperados = ['web_search', 'social_analysis', 'mental_drivers', 'future_predictions', 'avatar_detalhado']
            componentes_presentes = [comp for comp in componentes_esperados if comp in resultados]
            metricas['componentes_executados'] = componentes_presentes
            metricas['cobertura_analise'] = (len(componentes_presentes) / len(componentes_esperados)) * 100

            # Determina qualidade dos dados
            if metricas['cobertura_analise'] >= 80:
                metricas['qualidade_dados'] = 'alta'
            elif metricas['cobertura_analise'] >= 60:
                metricas['qualidade_dados'] = 'm√©dia'
            else:
                metricas['qualidade_dados'] = 'baixa'

        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair m√©tricas: {e}")

        return metricas

# Inst√¢ncia global
unified_analysis_engine = UnifiedAnalysisEngine()