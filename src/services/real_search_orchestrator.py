#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Real Search Orchestrator
Orquestrador de busca REAL massiva com rota√ß√£o de APIs e captura visual
"""

import os
import logging
import asyncio
import aiohttp
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import quote_plus
import json

logger = logging.getLogger(__name__)

class RealSearchOrchestrator:
    """Orquestrador de busca REAL massiva - ZERO SIMULA√á√ÉO"""

    def __init__(self):
        """Inicializa orquestrador com todas as APIs reais"""
        self.api_keys = self._load_all_api_keys()
        self.key_indices = {provider: 0 for provider in self.api_keys.keys()}

        # Provedores em ordem de prioridade
        self.providers = [
            'ALIBABA_WEBSAILOR',  # Adicionado como prioridade m√°xima
            'FIRECRAWL',
            'JINA', 
            'GOOGLE',
            'EXA',
            'SERPER',
            'YOUTUBE',
            'SUPADATA'
        ]

        # URLs base dos servi√ßos
        self.service_urls = {
            'FIRECRAWL': 'https://api.firecrawl.dev/v0/scrape',
            'JINA': 'https://r.jina.ai/',
            'GOOGLE': 'https://www.googleapis.com/customsearch/v1',
            'EXA': 'https://api.exa.ai/search',
            'SERPER': 'https://google.serper.dev/search',
            'YOUTUBE': 'https://www.googleapis.com/youtube/v3/search',
            'SUPADATA': os.getenv('SUPADATA_API_URL', 'https://server.smithery.ai/@supadata-ai/mcp/mcp')
        }

        self.session_stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'failed_searches': 0,
            'api_rotations': {},
            'content_extracted': 0,
            'screenshots_captured': 0
        }

        logger.info(f"üöÄ Real Search Orchestrator inicializado com {sum(len(keys) for keys in self.api_keys.values())} chaves totais")

    def _load_all_api_keys(self) -> Dict[str, List[str]]:
        """Carrega todas as chaves de API do ambiente"""
        api_keys = {}

        for provider in ['FIRECRAWL', 'JINA', 'GOOGLE', 'EXA', 'SERPER', 'YOUTUBE', 'SUPADATA', 'X']:
            keys = []

            # Chave principal
            main_key = os.getenv(f"{provider}_API_KEY")
            if main_key:
                keys.append(main_key)

            # Chaves numeradas
            counter = 1
            while True:
                numbered_key = os.getenv(f"{provider}_API_KEY_{counter}")
                if numbered_key:
                    keys.append(numbered_key)
                    counter += 1
                else:
                    break

            if keys:
                api_keys[provider] = keys
                logger.info(f"‚úÖ {provider}: {len(keys)} chaves carregadas")

        return api_keys

    def get_next_api_key(self, provider: str) -> Optional[str]:
        """Obt√©m pr√≥xima chave de API com rota√ß√£o autom√°tica"""
        if provider not in self.api_keys or not self.api_keys[provider]:
            return None

        keys = self.api_keys[provider]
        current_index = self.key_indices[provider]

        # Obt√©m chave atual
        key = keys[current_index]

        # Rotaciona para pr√≥xima
        self.key_indices[provider] = (current_index + 1) % len(keys)

        # Atualiza estat√≠sticas
        if provider not in self.session_stats['api_rotations']:
            self.session_stats['api_rotations'][provider] = 0
        self.session_stats['api_rotations'][provider] += 1

        logger.debug(f"üîÑ {provider}: Usando chave {current_index + 1}/{len(keys)}")
        return key

    async def execute_massive_real_search(
        self, 
        query: str, 
        context: Dict[str, Any],
        session_id: str
    ) -> Dict[str, Any]:
        """Executa busca REAL massiva com todos os provedores"""

        logger.info(f"üöÄ INICIANDO BUSCA REAL MASSIVA para: {query}")
        start_time = time.time()

        # Estrutura de resultados
        search_results = {
            'query': query,
            'session_id': session_id,
            'search_started': datetime.now().isoformat(),
            'providers_used': [],
            'web_results': [],
            'social_results': [],
            'youtube_results': [],
            'viral_content': [],
            'screenshots_captured': [],
            'statistics': {
                'total_sources': 0,
                'unique_urls': 0,
                'content_extracted': 0,
                'api_calls_made': 0,
                'search_duration': 0
            }
        }

        try:
            # FASE 1: Busca com Alibaba WebSailor (priorit√°ria)
            logger.info("üîç FASE 1: Busca com Alibaba WebSailor")
            websailor_results = await self._search_alibaba_websailor(query, context)
            
            if websailor_results.get('success'):
                search_results['web_results'].extend(websailor_results['results'])
                search_results['providers_used'].append('ALIBABA_WEBSAILOR')
                logger.info(f"‚úÖ Alibaba WebSailor retornou {len(websailor_results['results'])} resultados")

            # FASE 2: Busca Web Massiva Simult√¢nea (provedores restantes)
            logger.info("üåê FASE 2: Busca web massiva simult√¢nea")
            web_tasks = []

            # Firecrawl
            if 'FIRECRAWL' in self.api_keys:
                web_tasks.append(self._search_firecrawl(query))

            # Jina
            if 'JINA' in self.api_keys:
                web_tasks.append(self._search_jina(query))

            # Google
            if 'GOOGLE' in self.api_keys:
                web_tasks.append(self._search_google(query))

            # Exa
            if 'EXA' in self.api_keys:
                web_tasks.append(self._search_exa(query))

            # Serper
            if 'SERPER' in self.api_keys:
                web_tasks.append(self._search_serper(query))

            # Executa todas as buscas web simultaneamente
            if web_tasks:
                web_results = await asyncio.gather(*web_tasks, return_exceptions=True)

                for result in web_results:
                    if isinstance(result, Exception):
                        logger.error(f"‚ùå Erro na busca web: {result}")
                        continue

                    if result.get('success') and result.get('results'):
                        search_results['web_results'].extend(result['results'])
                        search_results['providers_used'].append(result.get('provider', 'unknown'))

            # FASE 3: Busca em Redes Sociais
            logger.info("üì± FASE 3: Busca massiva em redes sociais")
            social_tasks = []

            # YouTube
            if 'YOUTUBE' in self.api_keys:
                social_tasks.append(self._search_youtube(query))

            # Supadata (Instagram, Facebook, TikTok)
            # if 'SUPADATA' in self.api_keys:
            #     social_tasks.append(self._search_supadata(query))

            # Executa buscas sociais
            if social_tasks:
                social_results = await asyncio.gather(*social_tasks, return_exceptions=True)

                for result in social_results:
                    if isinstance(result, Exception):
                        logger.error(f"‚ùå Erro na busca social: {result}")
                        continue

                    if result.get('success'):
                        if result.get('platform') == 'youtube':
                            search_results['youtube_results'].extend(result.get('results', []))
                        else:
                            search_results['social_results'].extend(result.get('results', []))

            # FASE 4: Identifica√ß√£o de Conte√∫do Viral
            logger.info("üî• FASE 4: Identificando conte√∫do viral")
            viral_content = self._identify_viral_content(
                search_results['youtube_results'] + search_results['social_results']
            )
            search_results['viral_content'] = viral_content

            # FASE 5: Captura de Screenshots
            logger.info("üì∏ FASE 5: Capturando screenshots do conte√∫do viral")
            if viral_content:
                screenshots = await self._capture_viral_screenshots(viral_content, session_id)
                search_results['screenshots_captured'] = screenshots
                self.session_stats['screenshots_captured'] = len(screenshots)

            # Calcula estat√≠sticas finais
            search_duration = time.time() - start_time
            all_results = search_results['web_results'] + search_results['social_results'] + search_results['youtube_results']
            unique_urls = list(set(r.get('url', '') for r in all_results if r.get('url')))

            search_results['statistics'].update({
                'total_sources': len(all_results),
                'unique_urls': len(unique_urls),
                'content_extracted': sum(len(r.get('content', '')) for r in all_results),
                'api_calls_made': sum(self.session_stats['api_rotations'].values()),
                'search_duration': search_duration
            })

            logger.info(f"‚úÖ BUSCA REAL MASSIVA CONCLU√çDA em {search_duration:.2f}s")
            logger.info(f"üìä {len(all_results)} resultados de {len(search_results['providers_used'])} provedores")
            logger.info(f"üì∏ {len(search_results['screenshots_captured'])} screenshots capturados")

            return search_results

        except Exception as e:
            logger.error(f"‚ùå ERRO CR√çTICO na busca massiva: {e}")
            raise

    async def _search_alibaba_websailor(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Busca REAL usando Alibaba WebSailor Agent"""
        try:
            # Importa o agente WebSailor
            from services.alibaba_websailor import alibaba_websailor
            
            if not alibaba_websailor or not alibaba_websailor.enabled:
                logger.warning("‚ö†Ô∏è Alibaba WebSailor n√£o est√° habilitado")
                return {'success': False, 'error': 'Alibaba WebSailor n√£o habilitado'}

            # Executa a pesquisa profunda - CORRIGIDO: m√©todo s√≠ncrono, removendo await
            research_result = alibaba_websailor.navigate_and_research_deep(
                query=query,
                context=context,
                max_pages=30,
                depth_levels=2,
                session_id=None # Ou passe session_id se o m√©todo aceitar
            )

            if not research_result or not research_result.get('conteudo_consolidado'):
                return {'success': False, 'error': 'Nenhum resultado da pesquisa WebSailor'}

            # Converte resultados do WebSailor para formato padr√£o
            results = []
            fontes_detalhadas = research_result.get('conteudo_consolidado', {}).get('fontes_detalhadas', [])
            
            for fonte in fontes_detalhadas:
                results.append({
                    'title': fonte.get('title', ''),
                    'url': fonte.get('url', ''),
                    'snippet': '',  # WebSailor n√£o fornece snippet diretamente
                    'source': 'alibaba_websailor',
                    'relevance_score': fonte.get('quality_score', 0.7),
                    'content_length': fonte.get('content_length', 0)
                })

            logger.info(f"‚úÖ Alibaba WebSailor processado com {len(results)} resultados")
            
            return {
                'success': True,
                'provider': 'ALIBABA_WEBSAILOR',
                'results': results,
                'raw_data': research_result
            }

        except ImportError:
            logger.warning("‚ö†Ô∏è Alibaba WebSailor n√£o encontrado")
            return {'success': False, 'error': 'Alibaba WebSailor n√£o dispon√≠vel'}
        except Exception as e:
            logger.error(f"‚ùå Erro Alibaba WebSailor: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_firecrawl(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Firecrawl"""
        try:
            api_key = self.get_next_api_key('FIRECRAWL')
            if not api_key:
                return {'success': False, 'error': 'Firecrawl API key n√£o dispon√≠vel'}

            # Busca no Google e extrai com Firecrawl
            search_url = f"https://www.google.com/search?q={quote_plus(query)}&hl=pt-BR&gl=BR"

            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }

                payload = {
                    'url': search_url,
                    'formats': ['markdown', 'html'],
                    'onlyMainContent': True,
                    'includeTags': ['p', 'h1', 'h2', 'h3', 'article'],
                    'excludeTags': ['nav', 'footer', 'aside', 'script'],
                    'waitFor': 3000
                }

                async with session.post(
                    self.service_urls['FIRECRAWL'],
                    json=payload,
                    headers=headers,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data.get('data', {}).get('markdown', '')

                        # Extrai resultados do conte√∫do
                        results = self._extract_search_results_from_content(content, 'firecrawl')

                        return {
                            'success': True,
                            'provider': 'FIRECRAWL',
                            'results': results,
                            'raw_content': content[:2000]
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Firecrawl erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro Firecrawl: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_jina(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Jina AI"""
        try:
            api_key = self.get_next_api_key('JINA')
            if not api_key:
                return {'success': False, 'error': 'Jina API key n√£o dispon√≠vel'}

            # Busca m√∫ltiplas URLs com Jina
            search_urls = [
                f"https://www.google.com/search?q={quote_plus(query)}&hl=pt-BR",
                f"https://www.bing.com/search?q={quote_plus(query)}&cc=br",
                f"https://search.yahoo.com/search?p={quote_plus(query)}&ei=UTF-8"
            ]

            results = []

            async with aiohttp.ClientSession() as session:
                for search_url in search_urls:
                    try:
                        jina_url = f"{self.service_urls['JINA']}{search_url}"
                        headers = {
                            'Authorization': f'Bearer {api_key}',
                            'Accept': 'text/plain'
                        }

                        async with session.get(
                            jina_url,
                            headers=headers,
                            timeout=30
                        ) as response:
                            if response.status == 200:
                                content = await response.text()
                                extracted_results = self._extract_search_results_from_content(content, 'jina')
                                results.extend(extracted_results)

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro em URL Jina {search_url}: {e}")
                        continue

            return {
                'success': True,
                'provider': 'JINA',
                'results': results[:20]  # Limita a 20 resultados
            }

        except Exception as e:
            logger.error(f"‚ùå Erro Jina: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_google(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Google Custom Search"""
        try:
            api_key = self.get_next_api_key('GOOGLE')
            cse_id = os.getenv('GOOGLE_CSE_ID')

            if not api_key or not cse_id:
                return {'success': False, 'error': 'Google API n√£o configurada'}

            async with aiohttp.ClientSession() as session:
                params = {
                    'key': api_key,
                    'cx': cse_id,
                    'q': f"{query} Brasil 2024",
                    'num': 10,
                    'lr': 'lang_pt',
                    'gl': 'br',
                    'safe': 'off',
                    'dateRestrict': 'm6'
                }

                async with session.get(
                    self.service_urls['GOOGLE'],
                    params=params,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        for item in data.get('items', []):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'snippet': item.get('snippet', ''),
                                'source': 'google_real',
                                'published_date': item.get('pagemap', {}).get('metatags', [{}])[0].get('article:published_time', ''),
                                'relevance_score': 0.9
                            })

                        return {
                            'success': True,
                            'provider': 'GOOGLE',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Google erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro Google: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_youtube(self, query: str) -> Dict[str, Any]:
        """Busca REAL no YouTube com foco em conte√∫do viral"""
        try:
            api_key = self.get_next_api_key('YOUTUBE')
            if not api_key:
                return {'success': False, 'error': 'YouTube API key n√£o dispon√≠vel'}

            async with aiohttp.ClientSession() as session:
                params = {
                    'part': "snippet,id",
                    'q': f"{query} Brasil",
                    'key': api_key,
                    'maxResults': 25,
                    'order': 'viewCount',  # Ordena por visualiza√ß√µes
                    'type': 'video',
                    'regionCode': 'BR',
                    'relevanceLanguage': 'pt',
                    'publishedAfter': '2023-01-01T00:00:00Z'
                }

                async with session.get(
                    self.service_urls['YOUTUBE'],
                    params=params,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        for item in data.get('items', []):
                            snippet = item.get('snippet', {})
                            video_id = item.get('id', {}).get('videoId', '')

                            # Busca estat√≠sticas detalhadas
                            stats = await self._get_youtube_video_stats(video_id, api_key, session)

                            results.append({
                                'title': snippet.get('title', ''),
                                'url': f"https://www.youtube.com/watch?v={video_id}",
                                'description': snippet.get('description', ''),
                                'channel': snippet.get('channelTitle', ''),
                                'published_at': snippet.get('publishedAt', ''),
                                'thumbnail': snippet.get('thumbnails', {}).get('high', {}).get('url', ''),
                                'view_count': stats.get('viewCount', 0),
                                'comment_count': stats.get('commentCount', 0),
                                'platform': 'youtube',
                                'viral_score': self._calculate_viral_score(stats),
                                'relevance_score': 0.85
                            })

                        # Ordena por score viral
                        results.sort(key=lambda x: x['viral_score'], reverse=True)

                        return {
                            'success': True,
                            'provider': 'YOUTUBE',
                            'platform': 'youtube',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå YouTube erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro YouTube: {e}")
            return {'success': False, 'error': str(e)}

    async def _get_youtube_video_stats(self, video_id: str, api_key: str, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """Obt√©m estat√≠sticas detalhadas de um v√≠deo do YouTube"""
        try:
            params = {
                'part': 'statistics',
                'id': video_id,
                'key': api_key
            }

            async with session.get(
                'https://www.googleapis.com/youtube/v3/videos',
                params=params,
                timeout=10
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    items = data.get('items', [])
                    if items:
                        return items[0].get('statistics', {})

                return {}

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao obter stats do v√≠deo {video_id}: {e}")
            return {}

    async def _search_supadata(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Supadata MCP"""
        try:
            api_key = self.get_next_api_key('SUPADATA')
            if not api_key:
                return {'success': False, 'error': 'Supadata API key n√£o dispon√≠vel'}

            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }

                payload = {
                    'method': 'social_search',
                    'params': {
                        'query': query,
                        'platforms': ['instagram', 'facebook', 'tiktok'],
                        'limit': 50,
                        'sort_by': 'engagement',
                        'include_metrics': True
                    }
                }

                async with session.post(
                    self.service_urls['SUPADATA'],
                    json=payload,
                    headers=headers,
                    timeout=45
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        posts = data.get('result', {}).get('posts', [])
                        for post in posts:
                            results.append({
                                'title': post.get('caption', '')[:100],
                                'url': post.get('url', ''),
                                'content': post.get('caption', ''),
                                'platform': post.get('platform', 'social'),
                                'engagement_rate': post.get('engagement_rate', 0),
                                'likes': post.get('likes', 0),
                                'comments': post.get('comments', 0),
                                'shares': post.get('shares', 0),
                                'author': post.get('author', ''),
                                'published_at': post.get('published_at', ''),
                                'viral_score': self._calculate_social_viral_score(post),
                                'relevance_score': 0.8
                            })

                        return {
                            'success': True,
                            'provider': 'SUPADATA',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Supadata erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro Supadata: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_twitter(self, query: str) -> Dict[str, Any]:
        """Busca REAL no Twitter/X"""
        try:
            api_key = self.get_next_api_key('X')
            if not api_key:
                return {'success': False, 'error': 'X API key n√£o dispon√≠vel'}

            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                }

                params = {
                    'query': f"{query} lang:pt",
                    'max_results': 50,
                    'tweet.fields': 'public_metrics,created_at,author_id',
                    'user.fields': 'username,verified,public_metrics',
                    'expansions': 'author_id'
                }

                async with session.get(
                    'https://api.twitter.com/2/tweets/search/recent',
                    params=params,
                    headers=headers,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        tweets = data.get('data', [])
                        users = {user['id']: user for user in data.get('includes', {}).get('users', [])}

                        for tweet in tweets:
                            author = users.get(tweet.get('author_id', ''), {})
                            metrics = tweet.get('public_metrics', {})

                            results.append({
                                'title': tweet.get('text', '')[:100],
                                'url': f"https://twitter.com/i/status/{tweet.get('id')}",
                                'content': tweet.get('text', ''),
                                'platform': 'twitter',
                                'author': author.get('username', ''),
                                'author_verified': author.get('verified', False),
                                'retweets': metrics.get('retweet_count', 0),
                                'likes': metrics.get('like_count', 0),
                                'replies': metrics.get('reply_count', 0),
                                'quotes': metrics.get('quote_count', 0),
                                'published_at': tweet.get('created_at', ''),
                                'viral_score': self._calculate_twitter_viral_score(metrics),
                                'relevance_score': 0.75
                            })

                        return {
                            'success': True,
                            'provider': 'X',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå X/Twitter erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro X/Twitter: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_exa(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Exa Neural Search"""
        try:
            api_key = self.get_next_api_key('EXA')
            if not api_key:
                return {'success': False, 'error': 'Exa API key n√£o dispon√≠vel'}

            async with aiohttp.ClientSession() as session:
                headers = {
                    'x-api-key': api_key,
                    'Content-Type': 'application/json'
                }

                payload = {
                    'query': f"{query} Brasil mercado tend√™ncias",
                    'numResults': 15,
                    'useAutoprompt': True,
                    'type': 'neural',
                    'includeDomains': [
                        'g1.globo.com', 'exame.com', 'valor.globo.com',
                        'estadao.com.br', 'folha.uol.com.br', 'infomoney.com.br'
                    ],
                    'startPublishedDate': '2023-01-01'
                }

                async with session.post(
                    self.service_urls['EXA'],
                    json=payload,
                    headers=headers,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        for item in data.get('results', []):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('url', ''),
                                'snippet': item.get('text', '')[:300],
                                'source': 'exa_neural',
                                'score': item.get('score', 0),
                                'published_date': item.get('publishedDate', ''),
                                'relevance_score': item.get('score', 0.8)
                            })

                        return {
                            'success': True,
                            'provider': 'EXA',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Exa erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro Exa: {e}")
            return {'success': False, 'error': str(e)}

    async def _search_serper(self, query: str) -> Dict[str, Any]:
        """Busca REAL usando Serper"""
        try:
            api_key = self.get_next_api_key('SERPER')
            if not api_key:
                return {'success': False, 'error': 'Serper API key n√£o dispon√≠vel'}

            async with aiohttp.ClientSession() as session:
                headers = {
                    'X-API-KEY': api_key,
                    'Content-Type': 'application/json'
                }

                payload = {
                    'q': f"{query} Brasil mercado",
                    'gl': 'br',
                    'hl': 'pt',
                    'num': 15,
                    'autocorrect': True
                }

                async with session.post(
                    self.service_urls['SERPER'],
                    json=payload,
                    headers=headers,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = []

                        for item in data.get('organic', []):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'snippet': item.get('snippet', ''),
                                'source': 'serper_real',
                                'position': item.get('position', 0),
                                'relevance_score': 0.85
                            })

                        return {
                            'success': True,
                            'provider': 'SERPER',
                            'results': results
                        }
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Serper erro {response.status}: {error_text}")
                        return {'success': False, 'error': f'HTTP {response.status}'}

        except Exception as e:
            logger.error(f"‚ùå Erro Serper: {e}")
            return {'success': False, 'error': str(e)}

    def _extract_search_results_from_content(self, content: str, provider: str) -> List[Dict[str, Any]]:
        """Extrai resultados de busca do conte√∫do extra√≠do"""
        results = []

        if not content:
            return results

        # Divide o conte√∫do em se√ß√µes
        lines = content.split('\n')
        current_result = {}

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Detecta t√≠tulos (linhas com mais de 20 caracteres e sem URLs)
            if (len(line) > 20 and 
                not line.startswith('http') and 
                not line.startswith('www') and
                '.' not in line[:10]):

                # Salva resultado anterior se existir
                if current_result.get('title'):
                    results.append(current_result)

                # Inicia novo resultado
                current_result = {
                    'title': line,
                    'url': '',
                    'snippet': '',
                    'source': provider,
                    'relevance_score': 0.7
                }

            # Detecta URLs
            elif line.startswith(('http', 'www')):
                if current_result:
                    current_result['url'] = line

            # Detecta descri√ß√µes (linhas m√©dias)
            elif 50 <= len(line) <= 200 and current_result:
                current_result['snippet'] = line

        # Adiciona √∫ltimo resultado
        if current_result.get('title'):
            results.append(current_result)

        # Filtra resultados v√°lidos
        valid_results = [r for r in results if r.get('title') and len(r.get('title', '')) > 10]

        return valid_results[:15]  # M√°ximo 15 por provedor

    def _identify_viral_content(self, all_social_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identifica conte√∫do viral para captura de screenshots"""

        if not all_social_results:
            return []

        # Ordena por score viral
        sorted_content = sorted(
            all_social_results, 
            key=lambda x: x.get('viral_score', 0), 
            reverse=True
        )

        # Seleciona top 10 conte√∫dos virais
        viral_content = []
        seen_urls = set()

        for content in sorted_content:
            url = content.get('url', '')
            if url and url not in seen_urls and len(viral_content) < 10:
                viral_content.append(content)
                seen_urls.add(url)

        logger.info(f"üî• {len(viral_content)} conte√∫dos virais identificados")
        return viral_content

    async def _capture_viral_screenshots(self, viral_content: List[Dict[str, Any]], session_id: str) -> List[Dict[str, Any]]:
        """Captura screenshots do conte√∫do viral usando Selenium"""

        screenshots = []

        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from webdriver_manager.chrome import ChromeDriverManager

            # Configura Chrome em modo headless
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-gpu")

            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            # Cria diret√≥rio para screenshots
            screenshots_dir = f"analyses_data/files/{session_id}"
            os.makedirs(screenshots_dir, exist_ok=True)

            try:
                for i, content in enumerate(viral_content, 1):
                    try:
                        url = content.get('url', '')
                        if not url:
                            continue

                        logger.info(f"üì∏ Capturando screenshot {i}/10: {content.get('title', 'Sem t√≠tulo')}")

                        # Acessa a URL
                        driver.get(url)

                        # Aguarda carregamento
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )

                        # Aguarda renderiza√ß√£o completa
                        time.sleep(3)

                        # Captura screenshot
                        screenshot_path = f"{screenshots_dir}/viral_content_{i:02d}.png"
                        driver.save_screenshot(screenshot_path)

                        # Verifica se foi criado
                        if os.path.exists(screenshot_path) and os.path.getsize(screenshot_path) > 0:
                            screenshots.append({
                                'content_data': content,
                                'screenshot_path': screenshot_path,
                                'filename': f"viral_content_{i:02d}.png",
                                'url': url,
                                'title': content.get('title', ''),
                                'platform': content.get('platform', ''),
                                'viral_score': content.get('viral_score', 0),
                                'captured_at': datetime.now().isoformat()
                            })

                            logger.info(f"‚úÖ Screenshot {i} capturado: {screenshot_path}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Falha ao capturar screenshot {i}")

                    except Exception as e:
                        logger.error(f"‚ùå Erro ao capturar screenshot {i}: {e}")
                        continue

            finally:
                driver.quit()

        except ImportError:
            logger.error("‚ùå Selenium n√£o instalado - screenshots n√£o dispon√≠veis")
            return []
        except Exception as e:
            logger.error(f"‚ùå Erro na captura de screenshots: {e}")
            return []

        return screenshots

    def _calculate_viral_score(self, stats: Dict[str, Any]) -> float:
        """Calcula score viral para YouTube"""
        try:
            views = int(stats.get('viewCount', 0))
            likes = int(stats.get('likeCount', 0))
            comments = int(stats.get('commentCount', 0))

            # F√≥rmula viral: views + (likes * 10) + (comments * 20)
            viral_score = views + (likes * 10) + (comments * 20)

            # Normaliza para 0-10
            return min(10.0, viral_score / 100000)

        except:
            return 0.0

    def _calculate_social_viral_score(self, post: Dict[str, Any]) -> float:
        """Calcula score viral para redes sociais"""
        try:
            likes = int(post.get('likes', 0))
            comments = int(post.get('comments', 0))
            shares = int(post.get('shares', 0))
            engagement_rate = float(post.get('engagement_rate', 0))

            # F√≥rmula viral para redes sociais
            viral_score = (likes * 1) + (comments * 5) + (shares * 10) + (engagement_rate * 1000)

            # Normaliza para 0-10
            return min(10.0, viral_score / 10000)

        except:
            return 0.0

    def _calculate_twitter_viral_score(self, metrics: Dict[str, Any]) -> float:
        """Calcula score viral para Twitter"""
        try:
            retweets = int(metrics.get('retweet_count', 0))
            likes = int(metrics.get('like_count', 0))
            replies = int(metrics.get('reply_count', 0))
            quotes = int(metrics.get('quote_count', 0))

            # F√≥rmula viral para Twitter
            viral_score = (retweets * 10) + (likes * 2) + (replies * 5) + (quotes * 15)

            # Normaliza para 0-10
            return min(10.0, viral_score / 5000)

        except:
            return 0.0

    def get_session_statistics(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas da sess√£o atual"""
        return self.session_stats.copy()

# Inst√¢ncia global
real_search_orchestrator = RealSearchOrchestrator()
