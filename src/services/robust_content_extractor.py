#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Robust Content Extractor
Extrator multicamadas aprimorado com suporte a PDF e fallback robusto
"""

import time
import random
import os
import logging
import requests
import json
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urljoin, urlparse
import re
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from services.auto_save_manager import salvar_etapa, salvar_erro

# Imports condicionais para n√£o quebrar se n√£o estiver instalado
try:
    import trafilatura
    HAS_TRAFILATURA = True
except ImportError:
    HAS_TRAFILATURA = False

try:
    from readability import Document
    HAS_READABILITY = True
except ImportError:
    HAS_READABILITY = False

try:
    import newspaper
    from newspaper import Article
    HAS_NEWSPAPER = True
except ImportError:
    HAS_NEWSPAPER = False

try:
    from bs4 import BeautifulSoup
    HAS_BEAUTIFULSOUP = True
except ImportError:
    HAS_BEAUTIFULSOUP = False

try:
    import PyPDF2
    HAS_PYPDF2 = True
except ImportError:
    HAS_PYPDF2 = False

try:
    import pdfplumber
    HAS_PDFPLUMBER = True
except ImportError:
    HAS_PDFPLUMBER = False

try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

from services.url_resolver import url_resolver

logger = logging.getLogger(__name__)

class RobustContentExtractor:
    """Extrator de conte√∫do multicamadas e robusto com suporte aprimorado a PDF"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

        self.timeout = 30
        self.min_content_length = 200  # Reduzido de 500 para 200
        self.max_content_length = 50000  # 50K chars max

        # Estat√≠sticas dos extratores
        self.stats = {
            'trafilatura': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_TRAFILATURA},
            'readability': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_READABILITY},
            'newspaper': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_NEWSPAPER},
            'beautifulsoup': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_BEAUTIFULSOUP},
            'pdf_pypdf2': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_PYPDF2},
            'pdf_pdfplumber': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_PDFPLUMBER},
            'pdf_pymupdf': {'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0, 'available': HAS_PYMUPDF},
            'global': {
                'total_extractions': 0,
                'total_successes': 0,
                'total_failures': 0,
                'success_rate': 0.0
            }
        }

        logger.info("üîß Robust Content Extractor inicializado")
        logger.info(f"üìö Extratores dispon√≠veis: {self._get_available_extractors()}")

    def extract_content(self, url: str) -> Optional[str]:
        """
        Extrai conte√∫do usando m√∫ltiplos extratores em ordem de prioridade
        Agora com suporte aprimorado a PDF e melhor fallback
        """
        if not url or not url.startswith('http'):
            logger.error(f"‚ùå URL inv√°lida: {url}")
            return None

        try:
            start_time = time.time()
            self.stats['global']['total_extractions'] += 1

            logger.info(f"üîç Iniciando extra√ß√£o de: {url}")

            # 1. Resolve URL de redirecionamento
            resolved_url = url_resolver.resolve_redirect_url(url)
            if resolved_url != url:
                logger.info(f"üîÑ URL resolvida: {url} -> {resolved_url}")
                # Salva resolu√ß√£o de URL
                salvar_etapa("url_resolvida", {
                    "original": url,
                    "resolved": resolved_url
                }, categoria="pesquisa_web")
                url = resolved_url

            # Valida URL resolvida
            if not url.startswith('http'):
                logger.error(f"‚ùå URL resolvida inv√°lida: {url}")
                salvar_erro("url_invalida", ValueError(f"URL inv√°lida: {url}"))
                self.stats['global']['total_failures'] += 1
                self._update_global_stats()
                return None

            # 2. Verifica se √© PDF
            if self._is_pdf_url(url):
                logger.info("üìÑ Detectado PDF - usando extratores especializados")
                # PRIORIDADE M√ÅXIMA: PyMuPDF primeiro
                from services.pymupdf_client import pymupdf_client
                if pymupdf_client.is_available():
                    logger.info("üöÄ Usando PyMuPDF Pro com PRIORIDADE M√ÅXIMA")
                    pdf_result = pymupdf_client.extract_from_url(url)
                    if pdf_result.get('success') and pdf_result.get('text'):
                        content = pdf_result['text']
                        if self._validate_content(content, url):
                            salvar_etapa("extracao_pymupdf_pro", {
                                "url": url,
                                "content_length": len(content),
                                "pages": pdf_result.get('metadata', {}).get('pages', 0),
                                "extractor": "PyMuPDF_Pro_Priority"
                            }, categoria="pesquisa_web")
                            self.stats['global']['total_successes'] += 1
                            self._update_global_stats()
                            logger.info(f"‚úÖ PyMuPDF Pro SUCESSO: {len(content)} caracteres")
                            return content

                # Fallback para outros extratores de PDF
                content = self._extract_pdf_content(url)
                if content and self._validate_content(content, url):
                    # Salva extra√ß√£o de PDF bem-sucedida
                    salvar_etapa("extracao_pdf", {
                        "url": url,
                        "content_length": len(content),
                        "extractor": "pdf_specialized"
                    }, categoria="pesquisa_web")
                    self.stats['global']['total_successes'] += 1
                    self._update_global_stats()
                    return content

            # 3. Baixa conte√∫do HTML
            html_content = self._fetch_html(url)
            if not html_content:
                logger.error(f"‚ùå Falha ao baixar HTML para {url}")
                salvar_erro("download_html", Exception(f"Falha no download: {url}"))
                self.stats['global']['total_failures'] += 1
                self._update_global_stats()
                return None

            # Valida HTML m√≠nimo
            if len(html_content) < 500:
                logger.warning(f"‚ö†Ô∏è HTML muito pequeno: {len(html_content)} caracteres")
                # Continua tentando extrair, mas com expectativas baixas

            logger.info(f"üì• HTML baixado: {len(html_content)} caracteres")

            # 4. Verifica se √© p√°gina din√¢mica (JavaScript-heavy)
            if self._is_dynamic_page(html_content):
                logger.warning(f"‚ö†Ô∏è P√°gina din√¢mica detectada: {url}")
                # Tenta extra√ß√£o mais agressiva
                content = self._extract_dynamic_content(html_content, url)
                if content and self._validate_content(content, url):
                    # Salva extra√ß√£o din√¢mica bem-sucedida
                    salvar_etapa("extracao_dinamica", {
                        "url": url,
                        "content_length": len(content),
                        "extractor": "dynamic_specialized"
                    }, categoria="pesquisa_web")
                    self.stats['global']['total_successes'] += 1
                    self._update_global_stats()
                    return content

            # 5. Tenta extratores em ordem de prioridade
            extractors = [
                ('trafilatura', self._extract_with_trafilatura),
                ('readability', self._extract_with_readability),
                ('newspaper', self._extract_with_newspaper),
                ('beautifulsoup', self._extract_with_beautifulsoup)
            ]

            for extractor_name, extractor_func in extractors:
                if not self._is_extractor_available(extractor_name):
                    continue

                try:
                    logger.info(f"üîç Tentando extra√ß√£o com {extractor_name}...")
                    extractor_start = time.time()
                    self.stats[extractor_name]['usage_count'] += 1

                    content = extractor_func(html_content, url)
                    extractor_time = time.time() - extractor_start

                    if self._validate_content(content, url):
                        self.stats[extractor_name]['success'] += 1
                        self.stats[extractor_name]['total_time'] += extractor_time
                        self.stats['global']['total_successes'] += 1
                        self._update_global_stats()

                        # Salva extra√ß√£o bem-sucedida
                        salvar_etapa("extracao_sucesso", {
                            "url": url,
                            "extractor": extractor_name,
                            "content_length": len(content),
                            "extraction_time": extractor_time
                        }, categoria="pesquisa_web")

                        logger.info(f"‚úÖ Extra√ß√£o bem-sucedida com {extractor_name}: {len(content)} caracteres em {extractor_time:.2f}s")
                        return content
                    else:
                        self.stats[extractor_name]['failed'] += 1
                        logger.warning(f"‚ö†Ô∏è Conte√∫do insuficiente com {extractor_name}: {len(content) if content else 0} caracteres")

                except Exception as e:
                    self.stats[extractor_name]['failed'] += 1
                    logger.error(f"‚ùå Erro com {extractor_name}: {str(e)}")
                    salvar_erro(f"extrator_{extractor_name}", e, contexto={"url": url})
                    continue

            # 6. Fallback final - extra√ß√£o agressiva
            logger.warning(f"‚ö†Ô∏è Todos os extratores padr√£o falharam, tentando extra√ß√£o agressiva...")
            content = self._aggressive_fallback_extraction(html_content, url)
            if content and len(content) >= 100:  # Crit√©rio mais flex√≠vel para fallback
                logger.info(f"‚úÖ Extra√ß√£o agressiva bem-sucedida: {len(content)} caracteres")
                # Salva fallback bem-sucedido
                salvar_etapa("extracao_fallback", {
                    "url": url,
                    "content_length": len(content),
                    "extractor": "aggressive_fallback"
                }, categoria="pesquisa_web")
                self.stats['global']['total_successes'] += 1
                self._update_global_stats()
                return content

            # Todos os extratores falharam
            logger.error(f"‚ùå FALHA CR√çTICA: Todos os extratores falharam para {url}")
            salvar_erro("extracao_total_falha", Exception(f"Todos extratores falharam: {url}"))
            self.stats['global']['total_failures'] += 1
            self._update_global_stats()
            return None

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico na extra√ß√£o de {url}: {str(e)}")
            salvar_erro("extracao_critica", e, contexto={"url": url})
            self.stats['global']['total_failures'] += 1
            self._update_global_stats()
            return None

    def _is_pdf_url(self, url: str) -> bool:
        """Verifica se a URL aponta para um PDF"""
        return (url.lower().endswith('.pdf') or 
                'pdf' in url.lower() or 
                'application/pdf' in url.lower())

    def _extract_pdf_content(self, url: str) -> Optional[str]:
        """Extrai conte√∫do de PDF usando m√∫ltiplas estrat√©gias"""

        try:
            # Baixa o PDF
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()

            # Salva temporariamente
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(response.content)
                temp_path = temp_file.name

            try:
                # Tenta PDFPlumber primeiro (melhor para PDFs complexos)
                if HAS_PDFPLUMBER:
                    content = self._extract_pdf_with_pdfplumber(temp_path)
                    if content and len(content) > 100:
                        self.stats['pdf_pdfplumber']['success'] += 1
                        logger.info(f"‚úÖ PDF extra√≠do com PDFPlumber: {len(content)} caracteres")
                        return content
                    else:
                        self.stats['pdf_pdfplumber']['failed'] += 1

                # Tenta PyMuPDF se dispon√≠vel
                if HAS_PYMUPDF:
                    content = self._extract_pdf_with_pymupdf(temp_path)
                    if content and len(content) > 100:
                        self.stats['pdf_pymupdf']['success'] += 1
                        logger.info(f"‚úÖ PDF extra√≠do com PyMuPDF: {len(content)} caracteres")
                        return content
                    else:
                        self.stats['pdf_pymupdf']['failed'] += 1

                # Fallback para PyPDF2
                if HAS_PYPDF2:
                    content = self._extract_pdf_with_pypdf2(temp_path)
                    if content and len(content) > 100:
                        self.stats['pdf_pypdf2']['success'] += 1
                        logger.info(f"‚úÖ PDF extra√≠do com PyPDF2: {len(content)} caracteres")
                        return content
                    else:
                        self.stats['pdf_pypdf2']['failed'] += 1

                logger.error(f"‚ùå Falha na extra√ß√£o de PDF: {url}")
                return None

            finally:
                # Remove arquivo tempor√°rio
                try:
                    os.unlink(temp_path)
                except:
                    pass

        except Exception as e:
            logger.error(f"‚ùå Erro ao processar PDF {url}: {str(e)}")
            return None

    def _extract_pdf_with_pdfplumber(self, pdf_path: str) -> Optional[str]:
        """Extrai texto usando PDFPlumber"""
        try:
            import pdfplumber

            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

            return self._clean_content(text) if text else None

        except Exception as e:
            logger.error(f"Erro PDFPlumber: {e}")
            return None

    def _extract_pdf_with_pypdf2(self, pdf_path: str) -> Optional[str]:
        """Extrai texto usando PyPDF2"""
        try:
            import PyPDF2

            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"

            return self._clean_content(text) if text else None

        except Exception as e:
            logger.error(f"Erro PyPDF2: {e}")
            return None

    def _extract_pdf_with_pymupdf(self, pdf_path: str) -> Optional[str]:
        """Extrai texto usando PyMuPDF"""
        try:
            import fitz

            doc = fitz.open(pdf_path)
            text = ""

            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text:
                    text += page_text + "\n"

            doc.close()
            return self._clean_content(text) if text else None

        except Exception as e:
            logger.error(f"Erro PyMuPDF: {e}")
            return None

    def _is_dynamic_page(self, html: str) -> bool:
        """Verifica se √© p√°gina din√¢mica (JavaScript-heavy)"""
        if not html:
            return False

        # Indicadores de p√°gina din√¢mica
        dynamic_indicators = [
            'react', 'angular', 'vue.js', 'spa-',
            'document.write', 'innerHTML', 'createElement',
            'loading...', 'carregando...', 'please enable javascript',
            'javascript required', 'js-', 'ng-', 'v-'
        ]

        html_lower = html.lower()
        js_indicators = sum(1 for indicator in dynamic_indicators if indicator in html_lower)

        # Se tem muitos indicadores JS e pouco conte√∫do de texto
        text_content = BeautifulSoup(html, 'html.parser').get_text() if HAS_BEAUTIFULSOUP else html
        text_ratio = len(text_content.strip()) / len(html) if html else 0

        return js_indicators > 3 and text_ratio < 0.1

    def _extract_dynamic_content(self, html: str, url: str) -> Optional[str]:
        """Extra√ß√£o especializada para conte√∫do din√¢mico"""

        if not HAS_BEAUTIFULSOUP:
            return None

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Remove scripts e elementos din√¢micos
            for element in soup(['script', 'style', 'noscript', 'iframe']):
                element.decompose()

            # Busca por elementos com conte√∫do pr√©-renderizado
            content_selectors = [
                '[data-content]', '[data-text]', '.content-loaded',
                '.server-rendered', '.static-content', '.preloaded',
                'main', 'article', '.post-content', '.article-content',
                '.entry-content', '.page-content', '.text-content'
            ]

            extracted_content = []

            for selector in content_selectors:
                try:
                    elements = soup.select(selector)
                    for element in elements:
                        text = element.get_text(strip=True)
                        if len(text) > 50:  # Conte√∫do substancial
                            extracted_content.append(text)
                except:
                    continue

            if extracted_content:
                combined = '\n\n'.join(extracted_content)
                return self._clean_content(combined)

            # Fallback: extrai todo texto dispon√≠vel
            all_text = soup.get_text()
            return self._clean_content(all_text) if len(all_text) > 100 else None

        except Exception as e:
            logger.error(f"Erro na extra√ß√£o din√¢mica: {e}")
            return None

    def _aggressive_fallback_extraction(self, html: str, url: str) -> Optional[str]:
        """Extra√ß√£o agressiva como √∫ltimo recurso"""

        if not HAS_BEAUTIFULSOUP:
            return None

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Remove apenas elementos cr√≠ticos
            for element in soup(['script', 'style']):
                element.decompose()

            # Coleta todo texto dispon√≠vel
            all_text = soup.get_text()

            # Filtra linhas com conte√∫do significativo
            lines = all_text.split('\n')
            meaningful_lines = []

            for line in lines:
                line = line.strip()
                if (len(line) > 20 and  # Linha substancial
                    not re.match(r'^[\s\W]*$', line) and  # N√£o s√≥ espa√ßos/s√≠mbolos
                    not line.lower().startswith(('menu', 'nav', 'footer', 'header'))):  # N√£o navega√ß√£o
                    meaningful_lines.append(line)

            if meaningful_lines:
                content = '\n'.join(meaningful_lines)
                return self._clean_content(content)

            return None

        except Exception as e:
            logger.error(f"Erro na extra√ß√£o agressiva: {e}")
            return None

    def _fetch_html(self, url: str) -> Optional[str]:
        """Baixa conte√∫do HTML da URL com retry"""
        max_retries = 3

        for attempt in range(max_retries):
            try:
                response = self.session.get(
                    url,
                    timeout=self.timeout,
                    verify=False,  # Para evitar problemas de SSL
                    allow_redirects=True
                )

                response.raise_for_status()

                # Detecta encoding
                if response.encoding is None:
                    response.encoding = 'utf-8'

                html = response.text

                if len(html) < 500:
                    logger.warning(f"‚ö†Ô∏è HTML muito pequeno (tentativa {attempt + 1}): {len(html)} caracteres")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # Aguarda antes de tentar novamente
                        continue

                return html

            except requests.exceptions.Timeout:
                logger.warning(f"‚è∞ Timeout na tentativa {attempt + 1} para {url}")
                if attempt < max_retries - 1:
                    time.sleep(2 + random.uniform(0, 2))  # Delay aleat√≥rio
                    continue
            except Exception as e:
                logger.error(f"‚ùå Erro ao baixar {url} (tentativa {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 + random.uniform(0, 2))  # Delay aleat√≥rio
                    continue

        return None

    def _extract_with_trafilatura(self, html: str, url: str) -> Optional[str]:
        """Extrai com Trafilatura (prioridade 1) com configura√ß√µes aprimoradas"""
        if not HAS_TRAFILATURA:
            return None

        try:
            # Configura√ß√µes mais agressivas para trafilatura
            content = trafilatura.extract(
                html,
                include_comments=False,
                include_tables=True,
                include_formatting=False,
                favor_precision=False,  # Mudado para False para ser mais inclusivo
                favor_recall=True,      # Prioriza recuperar mais conte√∫do
                url=url,
                config=trafilatura.settings.use_config()
            )

            if content:
                content = self._clean_content(content)
                return content

            return None

        except Exception as e:
            logger.error(f"Erro Trafilatura: {e}")
            return None

    def _extract_with_readability(self, html: str, url: str) -> Optional[str]:
        """Extrai com Readability (prioridade 2) com configura√ß√µes aprimoradas"""
        if not HAS_READABILITY:
            return None

        try:
            # Configura√ß√µes mais inclusivas
            doc = Document(html, positive_keywords=['content', 'article', 'post', 'text', 'main'])
            content = doc.summary()

            if content:
                # Remove tags HTML
                if HAS_BEAUTIFULSOUP:
                    soup = BeautifulSoup(content, 'html.parser')
                    content = soup.get_text()
                else:
                    # Remove tags manualmente
                    content = re.sub(r'<[^>]+>', '', content)

                content = self._clean_content(content)
                return content

            return None

        except Exception as e:
            logger.error(f"Erro Readability: {e}")
            return None

    def _extract_with_newspaper(self, html: str, url: str) -> Optional[str]:
        """Extrai com Newspaper3k (prioridade 3) com configura√ß√µes aprimoradas"""
        if not HAS_NEWSPAPER:
            return None

        try:
            article = Article(url)
            article.set_html(html)
            article.parse()

            content = article.text
            if content:
                content = self._clean_content(content)
                return content

            return None

        except Exception as e:
            logger.error(f"Erro Newspaper: {e}")
            return None

    def _extract_with_beautifulsoup(self, html: str, url: str) -> Optional[str]:
        """Extrai com BeautifulSoup (fallback final) com estrat√©gia aprimorada"""
        if not HAS_BEAUTIFULSOUP:
            return None

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # Remove scripts e styles
            for script in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
                script.decompose()

            # Estrat√©gia em camadas para encontrar conte√∫do
            content_strategies = [
                # Estrat√©gia 1: Elementos sem√¢nticos
                lambda: self._extract_semantic_content(soup),
                # Estrat√©gia 2: Elementos por classe/ID
                lambda: self._extract_by_selectors(soup),
                # Estrat√©gia 3: Maior bloco de texto
                lambda: self._extract_largest_text_block(soup),
                # Estrat√©gia 4: Todo o body
                lambda: self._extract_full_body(soup)
            ]

            for strategy in content_strategies:
                try:
                    content = strategy()
                    if content and len(content) > 100:
                        return self._clean_content(content)
                except:
                    continue

            return None

        except Exception as e:
            logger.error(f"Erro BeautifulSoup: {e}")
            return None

    def _extract_semantic_content(self, soup) -> Optional[str]:
        """Extrai usando elementos sem√¢nticos HTML5"""
        semantic_elements = soup.find_all(['article', 'main', 'section'])

        if semantic_elements:
            content_parts = []
            for element in semantic_elements:
                text = element.get_text()
                if len(text) > 50:
                    content_parts.append(text)

            if content_parts:
                return '\n\n'.join(content_parts)

        return None

    def _extract_by_selectors(self, soup) -> Optional[str]:
        """Extrai usando seletores CSS comuns"""
        content_selectors = [
            '.content', '#content', '.post', '.article',
            '.entry', '.text', '.body', '.main-content',
            '.post-content', '.article-content', '.entry-content',
            '.page-content', '.text-content', '.story-content'
        ]

        for selector in content_selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    content_parts = []
                    for element in elements:
                        text = element.get_text()
                        if len(text) > 50:
                            content_parts.append(text)

                    if content_parts:
                        return '\n\n'.join(content_parts)
            except:
                continue

        return None

    def _extract_largest_text_block(self, soup) -> Optional[str]:
        """Encontra e extrai o maior bloco de texto"""
        all_divs = soup.find_all(['div', 'section', 'article'])

        largest_text = ""
        largest_size = 0

        for div in all_divs:
            text = div.get_text()
            if len(text) > largest_size:
                largest_size = len(text)
                largest_text = text

        return largest_text if largest_size > 100 else None

    def _extract_full_body(self, soup) -> Optional[str]:
        """Extrai todo o conte√∫do do body como √∫ltimo recurso"""
        body = soup.find('body')
        if body:
            return body.get_text()
        else:
            return soup.get_text()

    def _clean_content(self, content: str) -> str:
        """Limpa e normaliza o conte√∫do extra√≠do com melhorias"""
        if not content:
            return ""

        # Remove quebras de linha excessivas
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)

        # Remove espa√ßos excessivos
        content = re.sub(r'[ \t]+', ' ', content)

        # Remove caracteres de controle
        content = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)

        # Remove linhas muito curtas (provavelmente navega√ß√£o)
        lines = content.split('\n')
        meaningful_lines = []

        for line in lines:
            line = line.strip()
            if (len(line) > 10 and  # Linha substancial
                not re.match(r'^[\s\W]*$', line) and  # N√£o s√≥ s√≠mbolos
                not line.lower() in ['menu', 'home', 'contato', 'sobre', 'login']):  # N√£o navega√ß√£o
                meaningful_lines.append(line)

        content = '\n'.join(meaningful_lines)

        # Normaliza
        content = content.strip()

        # Limita tamanho
        if len(content) > self.max_content_length:
            content = content[:self.max_content_length] + "..."

        return content

    def _validate_content(self, content: str, url: str) -> bool:
        """Valida se o conte√∫do extra√≠do √© v√°lido com crit√©rios aprimorados"""
        if not content:
            logger.warning(f"‚ö†Ô∏è Conte√∫do vazio para {url}")
            return False

        # Verifica tamanho m√≠nimo mais rigoroso
        min_length = 500 if not self._is_pdf_url(url) else 200
        if len(content) < min_length:
            logger.warning(f"‚ö†Ô∏è Conte√∫do pequeno para {url}: {len(content)} < {self.min_content_length}")
            return False

        # Verifica densidade de palavras
        words = content.split()
        min_words = 100 if not self._is_pdf_url(url) else 50
        if len(words) < min_words:
            logger.warning(f"‚ö†Ô∏è Poucas palavras para {url}: {len(words)}")
            return False

        # Verifica se n√£o √© p√°gina de erro
        error_indicators = ['404', 'not found', 'p√°gina n√£o encontrada', 'erro', 'error',
                           'forbidden', 'access denied', 'unauthorized', 'service unavailable']
        content_lower = content.lower()

        error_count = sum(1 for indicator in error_indicators if indicator in content_lower)
        if error_count > 2 and len(content) < 2000:
            logger.warning(f"‚ö†Ô∏è Poss√≠vel p√°gina de erro para {url}: {error_count} indicadores")
            return False

        # Verifica densidade de conte√∫do portugu√™s
        common_words = ['o', 'a', 'de', 'da', 'do', 'e', 'em', 'um', 'uma', 'com', 'n√£o', 'para', 'que', 'se', '√©', 'ou']
        real_words = sum(1 for word in words if any(common in word.lower() for common in common_words))

        portuguese_ratio = real_words / len(words)
        if portuguese_ratio < 0.08:  # Pelo menos 8% de palavras em portugu√™s
            logger.warning(f"‚ö†Ô∏è Conte√∫do suspeito para {url}: poucos conectivos ({real_words}/{len(words)})")
            return False

        # Verifica se n√£o √© s√≥ navega√ß√£o/menu
        navigation_words = ['menu', 'home', 'contato', 'sobre', 'login', 'cadastro', 'produtos', 'servi√ßos']
        nav_count = sum(1 for word in words if word.lower() in navigation_words)
        nav_ratio = nav_count / len(words)

        if nav_ratio > 0.3:  # Mais de 30% s√£o palavras de navega√ß√£o
            logger.warning(f"‚ö†Ô∏è Muito conte√∫do de navega√ß√£o para {url}: {nav_ratio:.2%}")
            return False

        logger.info(f"‚úÖ Conte√∫do v√°lido para {url}: {len(content)} caracteres, {len(words)} palavras")
        return True

    def _is_extractor_available(self, extractor_name: str) -> bool:
        """Verifica se o extrator est√° dispon√≠vel"""
        return self.stats.get(extractor_name, {}).get('available', False)

    def _get_available_extractors(self) -> List[str]:
        """Retorna lista de extratores dispon√≠veis"""
        available = []
        for name, stats in self.stats.items():
            if name != 'global' and stats.get('available', False):
                available.append(name)
        return available

    def _update_global_stats(self):
        """Atualiza estat√≠sticas globais"""
        total = self.stats['global']['total_extractions']
        successes = self.stats['global']['total_successes']

        if total > 0:
            self.stats['global']['success_rate'] = (successes / total) * 100

        # Atualiza estat√≠sticas individuais dos extratores
        for extractor_name, stats in self.stats.items():
            if extractor_name == 'global':
                continue

            total_attempts = stats['success'] + stats['failed']
            if total_attempts > 0:
                stats['success_rate'] = (stats['success'] / total_attempts) * 100
                if stats['success'] > 0:
                    stats['avg_response_time'] = stats['total_time'] / stats['success']
                else:
                    stats['avg_response_time'] = 0
            else:
                stats['success_rate'] = 0
                stats['avg_response_time'] = 0

            # Adiciona raz√£o se n√£o dispon√≠vel
            if not stats['available']:
                if extractor_name == 'trafilatura' and not HAS_TRAFILATURA:
                    stats['reason'] = 'Biblioteca trafilatura n√£o instalada'
                elif extractor_name == 'readability' and not HAS_READABILITY:
                    stats['reason'] = 'Biblioteca readability-lxml n√£o instalada'
                elif extractor_name == 'newspaper' and not HAS_NEWSPAPER:
                    stats['reason'] = 'Biblioteca newspaper3k n√£o instalada'
                elif extractor_name == 'beautifulsoup' and not HAS_BEAUTIFULSOUP:
                    stats['reason'] = 'Biblioteca beautifulsoup4 n√£o instalada'
                elif extractor_name == 'pdf_pypdf2' and not HAS_PYPDF2:
                    stats['reason'] = 'Biblioteca PyPDF2 n√£o instalada'
                elif extractor_name == 'pdf_pdfplumber' and not HAS_PDFPLUMBER:
                    stats['reason'] = 'Biblioteca pdfplumber n√£o instalada'

    def get_extractor_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas dos extratores"""
        self._update_global_stats()
        return self.stats.copy()

    def reset_extractor_stats(self, extractor_name: Optional[str] = None):
        """Reset estat√≠sticas dos extratores"""
        if extractor_name and extractor_name in self.stats:
            if extractor_name != 'global':
                self.stats[extractor_name].update({
                    'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0,
                    'success_rate': 0, 'avg_response_time': 0
                })
            logger.info(f"üîÑ Reset estat√≠sticas do extrator: {extractor_name}")
        else:
            # Reset todas
            for extractor in self.stats:
                if extractor != 'global':
                    self.stats[extractor].update({
                        'success': 0, 'failed': 0, 'total_time': 0, 'usage_count': 0,
                        'success_rate': 0, 'avg_response_time': 0
                    })

            self.stats['global'] = {
                'total_extractions': 0,
                'total_successes': 0,
                'total_failures': 0,
                'success_rate': 0.0
            }
            logger.info("üîÑ Reset estat√≠sticas de todos os extratores")

    def batch_extract(self, urls: List[str], max_workers: int = 5) -> Dict[str, Optional[str]]:
        """Extrai conte√∫do de m√∫ltiplas URLs em paralelo"""
        results = {}

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.extract_content, url): url for url in urls}

            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    content = future.result()
                    results[url] = content
                except Exception as e:
                    logger.error(f"Erro na extra√ß√£o paralela de {url}: {e}")
                    results[url] = None

        return results

    def test_extraction(self, url: str) -> Dict[str, Any]:
        """Testa extra√ß√£o para uma URL espec√≠fica com detalhes"""
        start_time = time.time()

        result = {
            'url': url,
            'success': False,
            'content_length': 0,
            'extraction_time': 0,
            'extractor_used': None,
            'error': None,
            'content_preview': None
        }

        try:
            content = self.extract_content(url)
            extraction_time = time.time() - start_time

            if content:
                result.update({
                    'success': True,
                    'content_length': len(content),
                    'extraction_time': extraction_time,
                    'content_preview': content[:500] + '...' if len(content) > 500 else content
                })
            else:
                result.update({
                    'extraction_time': extraction_time,
                    'error': 'Nenhum conte√∫do extra√≠do'
                })

        except Exception as e:
            result.update({
                'extraction_time': time.time() - start_time,
                'error': str(e)
            })

        return result

    def clear_cache(self):
        """Limpa cache de sess√£o"""
        self.session.close()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        logger.info("üßπ Cache de extra√ß√£o limpo")

# Inst√¢ncia global
robust_content_extractor = RobustContentExtractor()