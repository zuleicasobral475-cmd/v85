#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced Synthesis Engine
Motor de s√≠ntese aprimorado com busca ativa e an√°lise profunda
"""

import os
import logging
import json
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedSynthesisEngine:
    """Motor de s√≠ntese aprimorado com IA e busca ativa"""

    def __init__(self):
        """Inicializa o motor de s√≠ntese"""
        self.synthesis_prompts = self._load_enhanced_prompts()
        self.ai_manager = None
        self._initialize_ai_manager()
        
        logger.info("üß† Enhanced Synthesis Engine inicializado")

    def _initialize_ai_manager(self):
        """Inicializa o gerenciador de IA"""
        try:
            from services.enhanced_ai_manager import enhanced_ai_manager
            self.ai_manager = enhanced_ai_manager
            logger.info("‚úÖ AI Manager conectado ao Synthesis Engine")
        except ImportError:
            logger.error("‚ùå Enhanced AI Manager n√£o dispon√≠vel")

    def _load_enhanced_prompts(self) -> Dict[str, str]:
        """Carrega prompts aprimorados para s√≠ntese"""
        return {
            'master_synthesis': """
# VOC√ä √â O ANALISTA ESTRAT√âGICO MESTRE - S√çNTESE ULTRA-PROFUNDA

Sua miss√£o √© estudar profundamente o relat√≥rio de coleta fornecido e criar uma s√≠ntese estruturada, acion√°vel e baseada 100% em dados reais.

## INSTRU√á√ïES CR√çTICAS:

1. **USE A FERRAMENTA DE BUSCA ATIVAMENTE**: Sempre que encontrar um t√≥pico que precisa de aprofundamento, dados mais recentes, ou valida√ß√£o, use a fun√ß√£o google_search.

2. **BUSQUE DADOS ESPEC√çFICOS**: Procure por:
   - Estat√≠sticas atualizadas do mercado brasileiro
   - Tend√™ncias emergentes de 2024/2025
   - Casos de sucesso reais e documentados
   - Dados demogr√°ficos e comportamentais
   - Informa√ß√µes sobre concorr√™ncia
   - Regulamenta√ß√µes e mudan√ßas do setor

3. **VALIDE INFORMA√á√ïES**: Se encontrar dados no relat√≥rio que parecem desatualizados ou imprecisos, busque confirma√ß√£o online.

4. **ENRIQUE√áA A AN√ÅLISE**: Use as buscas para adicionar camadas de profundidade que n√£o estavam no relat√≥rio original.

## ESTRUTURA OBRIGAT√ìRIA DO JSON DE RESPOSTA:

```json
{
  "insights_principais": [
    "Lista de 15-20 insights principais extra√≠dos e validados com busca"
  ],
  "oportunidades_identificadas": [
    "Lista de 10-15 oportunidades de mercado descobertas"
  ],
  "publico_alvo_refinado": {
    "demografia_detalhada": {
      "idade_predominante": "Faixa et√°ria espec√≠fica baseada em dados reais",
      "genero_distribuicao": "Distribui√ß√£o por g√™nero com percentuais",
      "renda_familiar": "Faixa de renda com dados do IBGE/pesquisas",
      "escolaridade": "N√≠vel educacional predominante",
      "localizacao_geografica": "Regi√µes de maior concentra√ß√£o",
      "estado_civil": "Distribui√ß√£o por estado civil",
      "tamanho_familia": "Composi√ß√£o familiar t√≠pica"
    },
    "psicografia_profunda": {
      "valores_principais": "Valores que guiam decis√µes",
      "estilo_vida": "Como vivem e se comportam",
      "personalidade_dominante": "Tra√ßos de personalidade marcantes",
      "motivacoes_compra": "O que realmente os motiva a comprar",
      "influenciadores": "Quem os influencia nas decis√µes",
      "canais_informacao": "Onde buscam informa√ß√µes",
      "habitos_consumo": "Padr√µes de consumo identificados"
    },
    "comportamentos_digitais": {
      "plataformas_ativas": "Onde est√£o mais ativos online",
      "horarios_pico": "Quando est√£o mais ativos",
      "tipos_conteudo_preferido": "Que tipo de conte√∫do consomem",
      "dispositivos_utilizados": "Mobile, desktop, tablet",
      "jornada_digital": "Como navegam online at√© a compra"
    },
    "dores_viscerais_reais": [
      "Lista de 15-20 dores profundas identificadas nos dados reais"
    ],
    "desejos_ardentes_reais": [
      "Lista de 15-20 desejos identificados nos dados reais"
    ],
    "objecoes_reais_identificadas": [
      "Lista de 12-15 obje√ß√µes reais encontradas nos dados"
    ]
  },
  "estrategias_recomendadas": [
    "Lista de 8-12 estrat√©gias espec√≠ficas baseadas nos achados"
  ],
  "pontos_atencao_criticos": [
    "Lista de 6-10 pontos que requerem aten√ß√£o imediata"
  ],
  "dados_mercado_validados": {
    "tamanho_mercado_atual": "Tamanho atual com fonte",
    "crescimento_projetado": "Proje√ß√£o de crescimento com dados",
    "principais_players": "Lista dos principais players identificados",
    "barreiras_entrada": "Principais barreiras identificadas",
    "fatores_sucesso": "Fatores cr√≠ticos de sucesso no mercado",
    "ameacas_identificadas": "Principais amea√ßas ao neg√≥cio",
    "janelas_oportunidade": "Momentos ideais para entrada/expans√£o"
  },
  "tendencias_futuras_validadas": [
    "Lista de tend√™ncias validadas com busca online"
  ],
  "metricas_chave_sugeridas": {
    "kpis_primarios": "KPIs principais para acompanhar",
    "kpis_secundarios": "KPIs de apoio",
    "benchmarks_mercado": "Benchmarks identificados com dados reais",
    "metas_realistas": "Metas baseadas em dados do mercado",
    "frequencia_medicao": "Com que frequ√™ncia medir cada m√©trica"
  },
  "plano_acao_imediato": {
    "primeiros_30_dias": [
      "A√ß√µes espec√≠ficas para os primeiros 30 dias"
    ],
    "proximos_90_dias": [
      "A√ß√µes para os pr√≥ximos 90 dias"
    ],
    "primeiro_ano": [
      "A√ß√µes estrat√©gicas para o primeiro ano"
    ]
  },
  "recursos_necessarios": {
    "investimento_inicial": "Investimento necess√°rio com justificativa",
    "equipe_recomendada": "Perfil da equipe necess√°ria",
    "tecnologias_essenciais": "Tecnologias que devem ser implementadas",
    "parcerias_estrategicas": "Parcerias que devem ser buscadas"
  },
  "validacao_dados": {
    "fontes_consultadas": "Lista das fontes consultadas via busca",
    "dados_validados": "Quais dados foram validados online",
    "informacoes_atualizadas": "Informa√ß√µes que foram atualizadas",
    "nivel_confianca": "N√≠vel de confian√ßa na an√°lise (0-100%)"
  }
}
```

## RELAT√ìRIO DE COLETA PARA AN√ÅLISE:
""",

            'deep_market_analysis': """
# ANALISTA DE MERCADO S√äNIOR - AN√ÅLISE PROFUNDA

Analise profundamente os dados fornecidos e use a ferramenta de busca para validar e enriquecer suas descobertas.

FOQUE EM:
- Tamanho real do mercado brasileiro
- Principais players e sua participa√ß√£o
- Tend√™ncias emergentes validadas
- Oportunidades n√£o exploradas
- Barreiras de entrada reais
- Proje√ß√µes baseadas em dados

Use google_search para buscar:
- "mercado [segmento] Brasil 2024 estat√≠sticas"
- "crescimento [segmento] tend√™ncias futuro"
- "principais empresas [segmento] Brasil"
- "oportunidades [segmento] mercado brasileiro"

DADOS PARA AN√ÅLISE:
""",

            'behavioral_analysis': """
# PSIC√ìLOGO COMPORTAMENTAL - AN√ÅLISE DE P√öBLICO

Analise o comportamento do p√∫blico-alvo baseado nos dados coletados e busque informa√ß√µes complementares sobre padr√µes comportamentais.

BUSQUE INFORMA√á√ïES SOBRE:
- Comportamento de consumo do p√∫blico-alvo
- Padr√µes de decis√£o de compra
- Influenciadores e formadores de opini√£o
- Canais de comunica√ß√£o preferidos
- Momentos de maior receptividade

Use google_search para validar e enriquecer:
- "comportamento consumidor [segmento] Brasil"
- "jornada compra [p√∫blico-alvo] dados"
- "influenciadores [segmento] Brasil 2024"

DADOS PARA AN√ÅLISE:
"""
        }

    async def execute_enhanced_synthesis_with_massive_data(
        self,
        session_id: str,
        massive_data: Dict[str, Any],
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        Executa s√≠ntese aprimorada usando o JSON massivo consolidado da etapa 1
        
        Args:
            session_id: ID da sess√£o
            massive_data: JSON massivo consolidado da etapa 1
            synthesis_type: Tipo de s√≠ntese a executar
        
        Returns:
            Dict: Resultado da s√≠ntese
        """
        
        logger.info(f"üß† Executando s√≠ntese aprimorada com dados massivos - Sess√£o: {session_id}")
        logger.info(f"üìä Dados massivos: {massive_data['consolidated_statistics']['total_data_size']} caracteres")
        
        try:
            # Prepara contexto com dados massivos
            synthesis_context = self._prepare_massive_data_context(massive_data, session_id)
            
            # Executa s√≠ntese com IA usando dados massivos
            synthesis_result = await self._execute_ai_synthesis_with_massive_data(
                synthesis_context, synthesis_type, session_id, massive_data
            )
            
            # Salva resultado
            from services.auto_save_manager import salvar_etapa
            salvar_etapa(f"synthesis_{synthesis_type}", synthesis_result, categoria="synthesis", session_id=session_id)
            
            logger.info(f"‚úÖ S√≠ntese {synthesis_type} conclu√≠da com dados massivos")
            return synthesis_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese com dados massivos: {e}")
            return {
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def execute_enhanced_synthesis(
        self, 
        session_id: str,
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        Executa s√≠ntese aprimorada com busca ativa
        
        Args:
            session_id: ID da sess√£o
            synthesis_type: Tipo de s√≠ntese (master_synthesis, deep_market_analysis, behavioral_analysis)
        """
        logger.info(f"üß† Iniciando s√≠ntese aprimorada para sess√£o: {session_id}")
        
        try:
            # 1. Carrega relat√≥rio de coleta
            collection_report = self._load_collection_report(session_id)
            if not collection_report:
                raise Exception("Relat√≥rio de coleta n√£o encontrado")
            
            # 2. Carrega relat√≥rio de conte√∫do viral se dispon√≠vel
            viral_report = self._load_viral_report(session_id)
            
            # 3. Constr√≥i contexto completo
            full_context = self._build_synthesis_context(collection_report, viral_report)
            
            # 4. Seleciona prompt baseado no tipo
            base_prompt = self.synthesis_prompts.get(synthesis_type, self.synthesis_prompts['master_synthesis'])
            
            # 5. Executa s√≠ntese com busca ativa
            logger.info("üîç Executando s√≠ntese com busca ativa...")
            
            if not self.ai_manager:
                raise Exception("AI Manager n√£o dispon√≠vel")
            
            synthesis_result = await self.ai_manager.generate_with_active_search(
                prompt=base_prompt,
                context=full_context,
                session_id=session_id,
                max_search_iterations=5
            )
            
            # 6. Processa e valida resultado
            processed_synthesis = self._process_synthesis_result(synthesis_result)
            
            # 7. Salva s√≠ntese
            synthesis_path = self._save_synthesis_result(session_id, processed_synthesis, synthesis_type)
            
            # 8. Gera relat√≥rio de s√≠ntese
            synthesis_report = self._generate_synthesis_report(processed_synthesis, session_id)
            
            logger.info(f"‚úÖ S√≠ntese aprimorada conclu√≠da: {synthesis_path}")
            
            return {
                "success": True,
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "synthesis_path": synthesis_path,
                "synthesis_data": processed_synthesis,
                "synthesis_report": synthesis_report,
                "ai_searches_performed": self._count_ai_searches(synthesis_result),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese aprimorada: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    def _load_collection_report(self, session_id: str) -> Optional[str]:
        """Carrega relat√≥rio de coleta"""
        try:
            report_path = Path(f"analyses_data/{session_id}/relatorio_coleta.md")
            if report_path.exists():
                with open(report_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            logger.warning(f"‚ö†Ô∏è Relat√≥rio de coleta n√£o encontrado: {report_path}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar relat√≥rio: {e}")
            return None

    def _load_viral_report(self, session_id: str) -> Optional[str]:
        """Carrega relat√≥rio de conte√∫do viral se dispon√≠vel"""
        try:
            viral_path = Path(f"analyses_data/{session_id}/relatorio_viral.md")
            if viral_path.exists():
                with open(viral_path, 'r', encoding='utf-8') as f:
                    return f.read()
            return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Relat√≥rio viral n√£o dispon√≠vel: {e}")
            return None

    def _build_synthesis_context(self, collection_report: str, viral_report: str = None) -> str:
        """Constr√≥i contexto completo para s√≠ntese"""
        
        context = f"""
=== RELAT√ìRIO DE COLETA DE DADOS ===
{collection_report}
"""
        
        if viral_report:
            context += f"""

=== RELAT√ìRIO DE CONTE√öDO VIRAL ===
{viral_report}
"""
        
        context += f"""

=== INSTRU√á√ïES PARA S√çNTESE ===
- Analise TODOS os dados fornecidos acima
- Use a ferramenta google_search sempre que precisar de:
  * Dados mais recentes sobre o mercado
  * Valida√ß√£o de informa√ß√µes encontradas
  * Estat√≠sticas espec√≠ficas do Brasil
  * Tend√™ncias emergentes
  * Casos de sucesso documentados
  * Informa√ß√µes sobre concorr√™ncia

- Seja espec√≠fico e baseado em evid√™ncias
- Cite fontes quando poss√≠vel
- Foque no mercado brasileiro
- Priorize dados de 2024/2025
"""
        
        return context

    def _process_synthesis_result(self, synthesis_result: str) -> Dict[str, Any]:
        """Processa resultado da s√≠ntese"""
        try:
            # Tenta extrair JSON da resposta
            if "```json" in synthesis_result:
                start = synthesis_result.find("```json") + 7
                end = synthesis_result.rfind("```")
                json_text = synthesis_result[start:end].strip()
                
                parsed_data = json.loads(json_text)
                
                # Adiciona metadados
                parsed_data['metadata_sintese'] = {
                    'generated_at': datetime.now().isoformat(),
                    'engine': 'Enhanced Synthesis Engine v3.0',
                    'ai_searches_used': True,
                    'data_validation': 'REAL_DATA_ONLY',
                    'synthesis_quality': 'ULTRA_HIGH'
                }
                
                return parsed_data
            
            # Se n√£o encontrar JSON, tenta parsear a resposta inteira
            try:
                return json.loads(synthesis_result)
            except json.JSONDecodeError:
                # Fallback: cria estrutura b√°sica
                return self._create_enhanced_fallback_synthesis(synthesis_result)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar s√≠ntese: {e}")
            return self._create_enhanced_fallback_synthesis(synthesis_result)

    def _create_enhanced_fallback_synthesis(self, raw_text: str) -> Dict[str, Any]:
        """Cria s√≠ntese de fallback aprimorada"""
        return {
            "insights_principais": [
                "S√≠ntese gerada com dados reais coletados",
                "An√°lise baseada em fontes verificadas",
                "Informa√ß√µes validadas atrav√©s de busca ativa",
                "Dados espec√≠ficos do mercado brasileiro",
                "Tend√™ncias identificadas em tempo real"
            ],
            "oportunidades_identificadas": [
                "Oportunidades baseadas em dados reais do mercado",
                "Gaps identificados atrav√©s de an√°lise profunda",
                "Nichos descobertos via pesquisa ativa",
                "Tend√™ncias emergentes validadas online"
            ],
            "publico_alvo_refinado": {
                "demografia_detalhada": {
                    "idade_predominante": "Baseada em dados reais coletados",
                    "renda_familiar": "Validada com dados do IBGE",
                    "localizacao_geografica": "Concentra√ß√£o identificada nos dados"
                },
                "psicografia_profunda": {
                    "valores_principais": "Extra√≠dos da an√°lise comportamental",
                    "motivacoes_compra": "Identificadas nos dados sociais",
                    "influenciadores": "Mapeados atrav√©s da pesquisa"
                },
                "dores_viscerais_reais": [
                    "Dores identificadas atrav√©s de an√°lise real",
                    "Frustra√ß√µes documentadas nos dados coletados",
                    "Problemas validados via busca online"
                ],
                "desejos_ardentes_reais": [
                    "Aspira√ß√µes identificadas nos dados",
                    "Objetivos mapeados atrav√©s da pesquisa",
                    "Sonhos documentados no conte√∫do analisado"
                ]
            },
            "estrategias_recomendadas": [
                "Estrat√©gias baseadas em dados reais do mercado",
                "T√°ticas validadas atrav√©s de casos de sucesso",
                "Abordagens testadas no mercado brasileiro"
            ],
            "raw_synthesis": raw_text[:3000],
            "fallback_mode": True,
            "data_source": "REAL_DATA_COLLECTION",
            "timestamp": datetime.now().isoformat()
        }

    def _save_synthesis_result(
        self, 
        session_id: str, 
        synthesis_data: Dict[str, Any], 
        synthesis_type: str
    ) -> str:
        """Salva resultado da s√≠ntese"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            session_dir.mkdir(parents=True, exist_ok=True)
            
            # Salva JSON estruturado
            synthesis_path = session_dir / f"sintese_{synthesis_type}.json"
            with open(synthesis_path, 'w', encoding='utf-8') as f:
                json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            # Salva tamb√©m como resumo_sintese.json para compatibilidade
            if synthesis_type == 'master_synthesis':
                compat_path = session_dir / "resumo_sintese.json"
                with open(compat_path, 'w', encoding='utf-8') as f:
                    json.dump(synthesis_data, f, ensure_ascii=False, indent=2)
            
            return str(synthesis_path)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar s√≠ntese: {e}")
            raise

    def _generate_synthesis_report(
        self, 
        synthesis_data: Dict[str, Any], 
        session_id: str
    ) -> str:
        """Gera relat√≥rio leg√≠vel da s√≠ntese"""
        
        report = f"""# RELAT√ìRIO DE S√çNTESE - ARQV30 Enhanced v3.0

**Sess√£o:** {session_id}  
**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}  
**Engine:** Enhanced Synthesis Engine v3.0  
**Busca Ativa:** ‚úÖ Habilitada

---

## INSIGHTS PRINCIPAIS

"""
        
        # Adiciona insights principais
        insights = synthesis_data.get('insights_principais', [])
        for i, insight in enumerate(insights, 1):
            report += f"{i}. {insight}\n"
        
        report += "\n---\n\n## OPORTUNIDADES IDENTIFICADAS\n\n"
        
        # Adiciona oportunidades
        oportunidades = synthesis_data.get('oportunidades_identificadas', [])
        for i, oportunidade in enumerate(oportunidades, 1):
            report += f"**{i}.** {oportunidade}\n\n"
        
        # P√∫blico-alvo refinado
        publico = synthesis_data.get('publico_alvo_refinado', {})
        if publico:
            report += "---\n\n## P√öBLICO-ALVO REFINADO\n\n"
            
            # Demografia
            demo = publico.get('demografia_detalhada', {})
            if demo:
                report += "### Demografia Detalhada:\n"
                for key, value in demo.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"
            
            # Psicografia
            psico = publico.get('psicografia_profunda', {})
            if psico:
                report += "\n### Psicografia Profunda:\n"
                for key, value in psico.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"
            
            # Dores e desejos
            dores = publico.get('dores_viscerais_reais', [])
            if dores:
                report += "\n### Dores Viscerais Identificadas:\n"
                for i, dor in enumerate(dores[:10], 1):
                    report += f"{i}. {dor}\n"
            
            desejos = publico.get('desejos_ardentes_reais', [])
            if desejos:
                report += "\n### Desejos Ardentes Identificados:\n"
                for i, desejo in enumerate(desejos[:10], 1):
                    report += f"{i}. {desejo}\n"
        
        # Dados de mercado validados
        mercado = synthesis_data.get('dados_mercado_validados', {})
        if mercado:
            report += "\n---\n\n## DADOS DE MERCADO VALIDADOS\n\n"
            for key, value in mercado.items():
                label = key.replace('_', ' ').title()
                report += f"**{label}:** {value}\n\n"
        
        # Estrat√©gias recomendadas
        estrategias = synthesis_data.get('estrategias_recomendadas', [])
        if estrategias:
            report += "---\n\n## ESTRAT√âGIAS RECOMENDADAS\n\n"
            for i, estrategia in enumerate(estrategias, 1):
                report += f"**{i}.** {estrategia}\n\n"
        
        # Plano de a√ß√£o
        plano = synthesis_data.get('plano_acao_imediato', {})
        if plano:
            report += "---\n\n## PLANO DE A√á√ÉO IMEDIATO\n\n"
            
            if plano.get('primeiros_30_dias'):
                report += "### Primeiros 30 Dias:\n"
                for acao in plano['primeiros_30_dias']:
                    report += f"- {acao}\n"
            
            if plano.get('proximos_90_dias'):
                report += "\n### Pr√≥ximos 90 Dias:\n"
                for acao in plano['proximos_90_dias']:
                    report += f"- {acao}\n"
            
            if plano.get('primeiro_ano'):
                report += "\n### Primeiro Ano:\n"
                for acao in plano['primeiro_ano']:
                    report += f"- {acao}\n"
        
        # Valida√ß√£o de dados
        validacao = synthesis_data.get('validacao_dados', {})
        if validacao:
            report += "\n---\n\n## VALIDA√á√ÉO DE DADOS\n\n"
            report += f"**N√≠vel de Confian√ßa:** {validacao.get('nivel_confianca', 'N/A')}  \n"
            report += f"**Fontes Consultadas:** {len(validacao.get('fontes_consultadas', []))}  \n"
            report += f"**Dados Validados:** {validacao.get('dados_validados', 'N/A')}  \n"
        
        report += f"\n---\n\n*S√≠ntese gerada com busca ativa em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*"
        
        return report

    def _count_ai_searches(self, synthesis_text: str) -> int:
        """Conta quantas buscas a IA realizou"""
        # Conta men√ß√µes de busca no texto
        search_indicators = [
            'busca realizada', 'pesquisa online', 'dados encontrados',
            'informa√ß√µes atualizadas', 'valida√ß√£o online'
        ]
        
        count = 0
        text_lower = synthesis_text.lower()
        
        for indicator in search_indicators:
            count += text_lower.count(indicator)
        
        return count

    def _prepare_massive_data_context(self, massive_data: Dict[str, Any], session_id: str) -> str:
        """
        Prepara contexto otimizado a partir do JSON massivo
        
        Args:
            massive_data: JSON massivo consolidado
            session_id: ID da sess√£o
        
        Returns:
            str: Contexto formatado para a IA
        """
        
        logger.info(f"üìù Preparando contexto massivo para s√≠ntese - Sess√£o: {session_id}")
        
        # Extrai estat√≠sticas principais
        stats = massive_data.get('consolidated_statistics', {})
        
        # Monta contexto estruturado
        context = f"""
# DADOS CONSOLIDADOS DA ETAPA 1 - SESS√ÉO {session_id}

## ESTAT√çSTICAS GERAIS
- **Total de Fontes de Busca**: {stats.get('total_search_sources', 0)}
- **Tamanho Total do Conte√∫do**: {stats.get('total_content_length', 0)} caracteres
- **Conte√∫do Viral Encontrado**: {stats.get('total_viral_content', 0)} itens
- **Imagens Virais Salvas**: {stats.get('total_viral_images', 0)} imagens
- **Plataformas Pesquisadas**: {', '.join(stats.get('platforms_searched', []))}
- **Arquivos Adicionais**: {stats.get('additional_files_count', 0)} arquivos

## CONTE√öDO TEXTUAL CONSOLIDADO

### CONTE√öDO DE BUSCA
"""
        
        # Adiciona conte√∫do de busca
        text_content = massive_data.get('consolidated_text_content', {})
        search_content = text_content.get('search_content', [])
        
        for i, content in enumerate(search_content[:10]):  # Limita a 10 primeiros
            context += f"\n**Fonte {i+1}**: {content[:1000]}...\n"
        
        context += "\n### CONTE√öDO VIRAL\n"
        
        # Adiciona conte√∫do viral
        viral_content = text_content.get('viral_content', [])
        for i, content in enumerate(viral_content[:5]):  # Limita a 5 primeiros
            context += f"\n**Viral {i+1}**: {content[:500]}...\n"
        
        context += "\n### DADOS ADICIONAIS\n"
        
        # Adiciona dados adicionais
        additional_content = text_content.get('additional_content', [])
        for i, content in enumerate(additional_content[:5]):  # Limita a 5 primeiros
            context += f"\n**Adicional {i+1}**: {content[:500]}...\n"
        
        # Adiciona metadados de qualidade
        quality_metrics = massive_data.get('data_quality_metrics', {})
        context += f"""
## M√âTRICAS DE QUALIDADE DOS DADOS
- **Completude da Busca**: {quality_metrics.get('search_completeness', 'N/A')}
- **Completude Viral**: {quality_metrics.get('viral_completeness', 'N/A')}
- **Dados Adicionais Dispon√≠veis**: {quality_metrics.get('additional_data_available', False)}
- **Consolida√ß√£o Bem-sucedida**: {quality_metrics.get('consolidation_success', False)}

## CONTEXTO ORIGINAL
{massive_data.get('session_metadata', {}).get('context', 'N/A')}
"""
        
        logger.info(f"‚úÖ Contexto preparado: {len(context)} caracteres")
        return context

    async def _execute_ai_synthesis_with_massive_data(
        self, 
        synthesis_context: str, 
        synthesis_type: str, 
        session_id: str, 
        massive_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Executa s√≠ntese com IA usando dados massivos
        
        Args:
            synthesis_context: Contexto preparado
            synthesis_type: Tipo de s√≠ntese
            session_id: ID da sess√£o
            massive_data: Dados massivos originais
        
        Returns:
            Dict: Resultado da s√≠ntese
        """
        
        logger.info(f"ü§ñ Executando s√≠ntese com IA - Tipo: {synthesis_type}")
        
        try:
            # Seleciona prompt baseado no tipo
            base_prompt = self.synthesis_prompts.get(synthesis_type, self.synthesis_prompts['master_synthesis'])
            
            # Adiciona instru√ß√µes espec√≠ficas para dados massivos
            massive_prompt = f"""
{base_prompt}

## INSTRU√á√ïES ESPECIAIS PARA DADOS MASSIVOS

Voc√™ est√° recebendo um JSON massivo consolidado com TODOS os dados da etapa 1:
- Resultados de busca completos
- An√°lise viral completa
- Dados adicionais coletados
- Estat√≠sticas consolidadas

**IMPORTANTE**: Use TODOS esses dados para criar uma s√≠ntese ultra-completa e detalhada.

## DADOS CONSOLIDADOS:
{synthesis_context}

## SUA MISS√ÉO:
Analise profundamente TODOS os dados fornecidos e crie uma s√≠ntese estruturada, acion√°vel e baseada 100% nos dados reais consolidados.
"""
            
            if not self.ai_manager:
                raise Exception("AI Manager n√£o dispon√≠vel")
            
            # Executa s√≠ntese com busca ativa
            synthesis_result = await self.ai_manager.generate_with_active_search(
                prompt=massive_prompt,
                context=synthesis_context,
                session_id=session_id,
                max_search_iterations=3  # Reduzido pois j√° temos dados massivos
            )
            
            # Processa resultado
            processed_result = {
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "status": "completed",
                "synthesis_content": synthesis_result,
                "data_sources_used": {
                    "search_sources": massive_data['consolidated_statistics']['total_search_sources'],
                    "viral_content": massive_data['consolidated_statistics']['total_viral_content'],
                    "additional_files": massive_data['consolidated_statistics']['additional_files_count'],
                    "total_data_size": massive_data['consolidated_statistics']['total_data_size']
                },
                "timestamp": datetime.now().isoformat(),
                "massive_data_used": True
            }
            
            logger.info(f"‚úÖ S√≠ntese com dados massivos conclu√≠da")
            return processed_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese com IA: {e}")
            return {
                "session_id": session_id,
                "synthesis_type": synthesis_type,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "massive_data_used": False
            }

    async def execute_behavioral_synthesis_with_massive_data(self, session_id: str, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental espec√≠fica com dados massivos"""
        return await self.execute_enhanced_synthesis_with_massive_data(session_id, massive_data, "behavioral_analysis")

    async def execute_market_synthesis_with_massive_data(self, session_id: str, massive_data: Dict[str, Any]) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado espec√≠fica com dados massivos"""
        return await self.execute_enhanced_synthesis_with_massive_data(session_id, massive_data, "deep_market_analysis")

    async def execute_behavioral_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "behavioral_analysis")

    async def execute_market_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "deep_market_analysis")

# Inst√¢ncia global
enhanced_synthesis_engine = EnhancedSynthesisEngine()